---
title:  "[KT Aivle 3기 AI] 16일차. 머신러닝 (3)"
author: JIHWAN PARK
categories: [AI & 데이터분석, AIVLE SCHOOL]
tag: [AIVLE SCHOOL, Machine Learning, Python]
math: true
date: 2023-02-22 18:00:00 +0900
last_modified_at: 2023-02-22 18:00:00 +0900
---
> KT Aivle School 3기 AI 16일차 
> - 강사 : 이장래 강사님
> - 주제 : 기본 알고리즘 중 KNN과 Decision Tree
> - 내용 :
>   - KNN과 Decision Tree 설명 및 실습

# 기본 알고리즘
## K-Nearest Neighbor
- k 최근접 이웃
- 회귀와 분류에 사용되는 매우 간단한 지도학습 알고리즘
- 다른 알고리즘에 비해 이해하기 쉽지만, 연산 속도가 느림
- 적절한 k를 찾는 것이 중요 (default: n_neighbors=5)
- k 값이 커질 수록 모델이 단순해진다
- <a href='https://github.com/Jihwan98/aivle_school/blob/main/2023.02.20_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/%EB%B0%B0%EC%9A%B0%EA%B8%B0/ML03_02_KNN(AirQuality).ipynb' target='_blank'>KNN 실습코드 airquality</a>
- <a href='https://github.com/Jihwan98/aivle_school/blob/main/2023.02.20_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/%EC%9D%B5%ED%9E%88%EA%B8%B0/%EC%8B%A4%EC%8A%B503_03_KNN(Diabetes).ipynb' target='_blank'>KNN 실습코드 diabetes</a>


### **유클리드 거리, 맨하튼 거리**
- 유클리드 거리 : 피타고라스 정리와 동일
- 맨하튼 거리 : 수직, 수평 방향으로만 움직임

→ 항상 유클리드거리 ≤ 맨허튼 거리

### **정규화(Scaling)**
- 거리와 관련된 알고리즘은 정규화가 필수적!!
  - 축 별로 거리가 달라서...
- 정규화는 항상 <mark>학습 데이터</mark>로 진행!!
  - 미래의 데이터가 현재의 데이터에 영향을 주면 안된다.

#### **Normalization**
$\large X_{norm} = \frac{x - x_{min}}{x_{max} - x_{min}}$

#### **Standardization**
$\large X_z = \frac{x-x_{mean}}{std}$

> 공식을 직접 사용하거나, sklearn이 제공하는 함수 사용

```python
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(x_train)
x_train = scaler.transform(x_train)
x_test = scaler.transform(x_test)
```

## Decision Tree
- 결정 트리
- 분석 과정이 직관적이며, 이해와 설명하기가 쉬움
- 스무고개처럼 의미 있는 질문을 먼저 하는 것이 중요
  - → <mark>정보 이득이 높은 방향</mark>으로!!
- 과적합으로 모델 성능이 떨어지기 쉽다 → 트리 크기를 제한하는 튜닝이 필요
- <a href='https://github.com/Jihwan98/aivle_school/blob/main/2023.02.20_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/%EB%B0%B0%EC%9A%B0%EA%B8%B0/ML03_03_%EA%B2%B0%EC%A0%95%ED%8A%B8%EB%A6%AC(Titanic).ipynb' target='_blank'>Decision Tree 실습코드 titanic</a>

### **불순도 (Impurity)**
- 순도의 반대말
- 불순도를 수치화 할 수 있는 지표
  - 지니 불순도(Gini Impurity)
  - 엔트로피(Entropy)

### **지니 불순도 (Gini Impurity)**
- $\large Gini = 1 - \sum_{i=1}^c (p_i)^2$

- $1-\text{양성 클래스 비율}^2 + \text{음성 클래스 비율}^2$
- 지니 불순도의 값 : 0 ~ 0.5 (완벽하게 분류되면 : 0, 완벽하게 섞이면 : 0.5)
- 지니 불순도가 낮은 속성으로 의사결정 트리 노드 결정
- 트리를 내려가면서 불순도가 낮아지면 좋다!

### **엔트로피(Entropy)**
- $\large Entropy = -\sum_{i=1}^m p_i\log_2 p_i$

- 엔트로피
  - 음성클래스비율 $\times \log_2$(음성클래스비율)
  - 양성클래스비율 $\times \log_2$(양성클래스비율)
- 엔트로피 값 : 0 ~ 1 (완벽하게 분류되면 : 0, 완벽하게 섞이면 : 1)

### **정보 이득 (Information Gain)**
- $\large Gain(T, X) = Entropy(T) - Entropy(T, X)$
- 정보 이득이 크다 = 어떤 속성으로 분할할 때 불순도가 줄어든다.
- 모든 속성에 대해 분할한 후 정보 이득 계산
- 정보 이득이 가장 큰 속성부터 분할

### **가지치기 (튜닝)**
- 가지치기를 하지 않으면 Overfitting 발생 가능성 매우 높음
- 여러 하이퍼파라미터 값을 조정해 가지치기 할 수 있음
  - **max_depth**, min_samples_leaf, min_samples_split, max_feature, max_leaf_node 등
- 가장 적절한 하이퍼파라미터 값을 찾도록 노력해야 함

### **결정트리 시각화**
- 여러 시각화 방법 중 Graphvis 패키지 사용
- 사전에 Graphvis 패키지 설치 및 운영체제 환경 설정이 진행 되어야 한다.
- 코드는 실습 코드 확인
