---
title: '[KT Aivle 3기 AI] 8주차 (33~37일차). 미니프로젝트 3차'
author: JIHWAN PARK
categories:
  - AI & 데이터분석
  - AIVLE SCHOOL
tag:
  - AIVLE SCHOOL
  - Deep Learning
  - Project
math: true
date: '2023-03-22 16:45:55 +0900'
last_modified_at: '2023-03-22 16:45:55 +0900'
published: true
---
> KT Aivle School 3기 8주차. 미니프로젝트 3차 (1), (2)
{: .prompt-info}

# 👏 미니프로젝트 3차!
이번 주는 통째로 미니프로젝트를 진행했다. 주제는 2개로 월 ~ 수 / 목 ~ 금 각각 다른 주제로 진행되었다. 저번주에 배운 시각지능 딥러닝을 토대로 실제 Task를 해본다는 의미였는데, 직접 구현해보면서 스스로도 많은 정보들을 찾아보고 조원분들과 토의하면서 많은 성장을 할 수 있었던 것 같다!

## 💡 미니프로젝트 3차 (1) (33일 ~ 37일차)
- 강사 : 박상혁 강사님
- 주제 : 차량 공유 업체의 차량 파손 여부 분류
- 데이터 : DALL-E로 자체 제작된 데이터
<br>

- 데이터를 통일된 폴더구조로 정리한다.
- 딥러닝 모델을 직접 설계하고 학습을 진행해본다.
- Transfer Learning을 진행해본다
- Data Augmentation을 통해 학습해본다.

<details>
<summary>🌟 미니프로젝트 3차 (1) TIL</summary>
<div markdown="1">

## ✅ 데이터 폴더구조
- 먼저 데이터를 각각 train, validation, test로 나누고 keras의 flow_from_directory를 이용하기 위해서 폴더 구조를 동일하게 생성해야했다.
- 즉, 각각 폴더 안에 class 명의 폴더가 있고 그 폴더 안에 각 데이터가 있어야 했다.
- 강사님의 가이드 코드에서는 `os`와 `shutil`등을 이용해서 직접 파일을 옮기고 하는 과정이 있었고, 나는 이것이 익숙해서 바로 진행하였다.
- 근데, `splitfolders`라는 좋은 라이브러리가 있어서 쉽게 데이터를 정리할 수 있는 방법이 있었다!!

## ✅ 모델 설계 및 학습 과정에서 배운 점
- 현재 우리의 데이터는 양도 적었고, 너무 확실한 데이터였다(구분이 너무나도 잘 되는 데이터).
- 따라서, 오히려 모델을 깊게 쌓았을 때 보다 얕게 쌓았을 때 더 좋은 성능을 보여주는 현상을 확인할 수 있었다.
- 그리고 validation accuracy가 0.5에서 변하지 않는 현상이 있었는데, 이는 EarlyStopping의 Patience를 늘려주어 epoch를 많이 돌게하면 학습이 되었다. 그리고 필터 수, 노트 수 등등 모델 구조를 변경해서 파라미터 수를 조절하는 방법으로 해결할 수 있었다.
- 애초에 데이터 자체가 너무 명확해서 어떠한 모델을 써도 결과가 잘 나올 수 있었지만, 나의 경우는 EfficientNet을 사용했을 때 성능이 가장 좋았다.

### ✨ BatchNormalization
- 사실 이번 프로젝트에서 개인적으로 가장 큰 수확 중 하나는 BatchNormalization에 대해서 많이 찾아보고 공부해본 부분이다.
- 우리 조원분 중 한 분이 Transfer Learning을 할 때, Pretrained Model의 BatchNormalization Layer(이후 BN layer으로 부르겠습니다)를 Trainable 하게 하면 성능이 좋아진다는 글을 보셨고, 직접 해봤을 때도 성능이 좋아졌다는 말을 해주셨다.
- EfficientNet에 처음에는 끝 단만 학습하도록 했을 때, 학습이 되지 않았었다.
- 그래서 조원 분의 말을 듣고, BN layer를 Trainable 하게 했더니, 성능이 대폭 향상되어 심지어 Test Data에서 Accuracy 1이 나왔다.(실전 데이터에서는 일어날 수 없는 일임. 아마도 생성된 데이터다보니, train data에 있는 어떤 vector space가 test data에도 거의 비슷하게 존재하는 것으로 보임)
- 이 부분이 너무 궁금해서 엄청나게 찾아봤는데, 내 생각은 아래와 같다.

<details>
<summary>✔ <mark>Pretrained Model의 BatchNormalization Layer를 학습하는 것에 대하여..</mark></summary>
<div markdown="1">
기본적으로 Batch Normalization이 분포를 조절해주는 것인데, BN 을 학습하고 안하고의 차이가 큰 이유는 Imagenet의 분포와 지금 우리가 하고 있는 데이터의 분포의 차이가 커서 그렇지 않을까 하는 생각이 든다.

Tensorflow 공식문서(5번 링크)나 자료들을 찾아보면(여담으로 Chat GPT에서도) Transfer Learning을 할 때는 BN을 학습시키지 않는다는 말이 있다.

Tensorflow 공식 문서에서는 BN은 2개의 Trainable weights(알파, 베타를 의미 하는 것 같다)와 2개의 non Trainable weights(mini batch의 평균, 분산인 것 같다)가 있다고 하는데, 이 때문에 새롭게 학습을 진행하면 기존에 학습했던 것들이 파괴된다고 한다.

Chat GPT에서는 Pretrained Model이 이미 새로운 작업에도 유용할수 있도록 일반적인 Feature를 가지고 있고, BN layer는 이전 layer의 activation을 정규화 하기 위한 것이므로 새 작업을 위해 조정할 필요가 없을 수 있다고 말했다.

근데 내 개인적인 생각으로는 결국 학습시키고자 하는 데이터의 분포를 학습하는게 맞지 않을까 하는 생각인데, 3, 4번 링크에서 보면 BN을 train 하냐 안하냐는 정답은 없는 것 같다.

일단 내 결과에서는 BN을 학습시키는게 좋았다. 그 이유에 대해서 내 개인적인 생각은, Train data의 분포와 Validation, Test Data의 분포가 차이가 많이나서(이걸 Internal Covariate Shift 라고 한다) 그렇지 않을까 생각한다.

++ 관련하여 참고한 링크
1. [https://gaussian37.github.io/dl-concept-batchnorm/](https://gaussian37.github.io/dl-concept-batchnorm/)
2. [https://velog.io/@dltjrdud37/Batch-Normalization](https://velog.io/@dltjrdud37/Batch-Normalization)
3. [https://velog.io/@dlskawns/Deep-Learning-전이학습Transfer-Learning의-미세조정Fine-Tuning-방법-및-괴현상-원인분석-feat.-BN-ResNet50](https://velog.io/@dlskawns/Deep-Learning-%EC%A0%84%EC%9D%B4%ED%95%99%EC%8A%B5Transfer-Learning%EC%9D%98-%EB%AF%B8%EC%84%B8%EC%A1%B0%EC%A0%95Fine-Tuning-%EB%B0%A9%EB%B2%95-%EB%B0%8F-%EA%B4%B4%ED%98%84%EC%83%81-%EC%9B%90%EC%9D%B8%EB%B6%84%EC%84%9D-feat.-BN-ResNet50)
4. [https://stackoverflow.com/questions/63352974/tensorflow-transfer-learning-with-pre-trained-model-that-uses-batch-normalizatio](https://stackoverflow.com/questions/63352974/tensorflow-transfer-learning-with-pre-trained-model-that-uses-batch-normalizatio)
5. [https://www.tensorflow.org/guide/keras/transfer_learning?hl=ko](https://www.tensorflow.org/guide/keras/transfer_learning?hl=ko)



</div>
</details>

### ✨ EfficientNet
- 마찬가지로 이번 프로젝트에서 개인적으로 가장 큰 수확 중 하나는 EfficientNet에 대해서 공부해본 부분이다.
- 이름은 들어봤어도, 따로 공부해보거나 사용해보진 않았었다.
- EfficientNet이 성능이 좋다고 해서 관련해서 이해하고 사용해보기 위해서 논문리뷰 등 설명하는 자료들을 많이 살펴보았다.

### ✨ 기타 다른 부분들
1. <u>Global Average Pooling<u>
- GAP는 위치정보를 적게 잃어버리고, 파라미터 수를 크게 줄여 과적합을 방지할 수 있다.
- 이번 모델링에서는 GAP를 쓴 경우가 성능이 더 좋았다.
2. <u>Validation Data에서 Augmentaion을 하는 것이 성능 향상에 좋을까?</u>
- 확실하진 않지만, 오히려 Overfitting이 날 수 있다고 함.
3. <u>Random Seed를 고정하자.</u>
- 딥러닝 모델은 Random Seed를 고정할 부분이 너무 많아 모델을 재현하기 힘든데, 그래도 어느정도 Random Seed를 고정하고 모델이 수렴할 때까지 학습을 하면 크게 영향을 받지 않는다. (kernel_initializer 부분이 layer 별로 최초의 weight를 랜덤하게 설정)
- Random Seed 관련 코드 참고한 블로그 : [https://velog.io/@jhlee508/Code-Reproduction을-위한-Seed-고정-feat.-OS-Numpy-Random-PyTorch-Tensorflow](https://velog.io/@jhlee508/Code-Reproduction%EC%9D%84-%EC%9C%84%ED%95%9C-Seed-%EA%B3%A0%EC%A0%95-feat.-OS-Numpy-Random-PyTorch-Tensorflow)
4. <u>Batch Size와 Learning Rate</u>
- 관련한 포스팅을 통해 관계를 확인할 수 있었다 :[https://inhovation97.tistory.com/32](https://inhovation97.tistory.com/32)
- Batch Size에 따른 학습 관계에 대해서도 검색을 통해 알 수 있었는데, Batch Size가 크면 클 수록 train data에 Overfitting이 될 확률이 높고, 작을 수록 Robust한 모델이 학습될 가능성이 높다고 한다.
- 이유는 Batch Size가 클 수록 weight 값도 작아지고 전체를 반영하는 값으로 학습이 되고, 작을 수록 각각의 데이터에 대한 학습이 이루어지기 때문에 weight도 커지고 다양한 데이터에 대해 학습을 할 수 있다고 한다.

</div>
</details>
