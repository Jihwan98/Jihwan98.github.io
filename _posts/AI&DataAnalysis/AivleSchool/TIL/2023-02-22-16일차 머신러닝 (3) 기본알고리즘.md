---
title:  "[KT Aivle 3기 AI] 16일차. 머신러닝 (3) 기본알고리즘"
author: JIHWAN PARK
categories: [AI & 데이터분석, AIVLE SCHOOL]
tag: [AIVLE SCHOOL, Machine Learning, Python]
math: true
date: 2023-02-22 18:00:00 +0900
last_modified_at: 2023-03-03 18:00:00 +0900
---
> KT Aivle School 3기 AI 16일차 
> - 강사 : 이장래 강사님
> - 주제 : 기본 알고리즘 Linear Regression, KNN, Decision Tree, Logistic Regression, SVM
> - 내용 :
>   - Linear Regression, KNN, Decision Tree, Logistic Regression, SVM 설명 및 실습

# 기본 알고리즘
## **Linear Regression**
- 단순 회귀 : x 값 하나만으로 y값을 설명할 수 있는 경우
- 다중 회귀 : y 값을 서령하기 위해서는 여러 개의 x 값이 필요한 경우
- 회귀 계수 : `model.coef_`
- 편향 : `model.intercept_`
- <a href='https://github.com/Jihwan98/aivle_school/blob/main/2023.02.20_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/%EB%B0%B0%EC%9A%B0%EA%B8%B0/ML03_01_%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80(Cars).ipynb' target='_blank'>선형 회귀 실습 코드</a>

## K-Nearest Neighbor
- k 최근접 이웃
- 회귀와 분류에 사용되는 매우 간단한 지도학습 알고리즘
- 다른 알고리즘에 비해 이해하기 쉽지만, 연산 속도가 느림
- 적절한 k를 찾는 것이 중요 (default: n_neighbors=5)
- k 값이 커질 수록 모델이 단순해진다
- <a href='https://github.com/Jihwan98/aivle_school/blob/main/2023.02.20_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/%EB%B0%B0%EC%9A%B0%EA%B8%B0/ML03_02_KNN(AirQuality).ipynb' target='_blank'>KNN 실습코드 airquality</a>
- <a href='https://github.com/Jihwan98/aivle_school/blob/main/2023.02.20_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/%EC%9D%B5%ED%9E%88%EA%B8%B0/%EC%8B%A4%EC%8A%B503_03_KNN(Diabetes).ipynb' target='_blank'>KNN 실습코드 diabetes</a>


### **유클리드 거리, 맨하튼 거리**
- 유클리드 거리 : 피타고라스 정리와 동일
- 맨하튼 거리 : 수직, 수평 방향으로만 움직임

→ 항상 유클리드거리 ≤ 맨허튼 거리

### **정규화(Scaling)**
- 거리와 관련된 알고리즘은 정규화가 필수적!!
  - 축 별로 거리가 달라서...
- 정규화는 항상 <mark>학습 데이터</mark>로 진행!!
  - 미래의 데이터가 현재의 데이터에 영향을 주면 안된다.

#### **Normalization**
$\large X_{norm} = \frac{x - x_{min}}{x_{max} - x_{min}}$

#### **Standardization**
$\large X_z = \frac{x-x_{mean}}{std}$

> 공식을 직접 사용하거나, sklearn이 제공하는 함수 사용

```python
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(x_train)
x_train = scaler.transform(x_train)
x_test = scaler.transform(x_test)
```

## Decision Tree
- 결정 트리
- 분석 과정이 직관적이며, 이해와 설명하기가 쉬움
- 스무고개처럼 의미 있는 질문을 먼저 하는 것이 중요
  - → <mark>정보 이득이 높은 방향</mark>으로!!
- 과적합으로 모델 성능이 떨어지기 쉽다 → 트리 크기를 제한하는 튜닝이 필요
- <a href='https://github.com/Jihwan98/aivle_school/blob/main/2023.02.20_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/%EB%B0%B0%EC%9A%B0%EA%B8%B0/ML03_03_%EA%B2%B0%EC%A0%95%ED%8A%B8%EB%A6%AC(Titanic).ipynb' target='_blank'>Decision Tree 실습코드 titanic</a>

### **불순도 (Impurity)**
- 순도의 반대말
- 불순도를 수치화 할 수 있는 지표
  - 지니 불순도(Gini Impurity)
  - 엔트로피(Entropy)

### **지니 불순도 (Gini Impurity)**
- $\large Gini = 1 - \sum_{i=1}^c (p_i)^2$

- $1-\text{양성 클래스 비율}^2 + \text{음성 클래스 비율}^2$
- 지니 불순도의 값 : 0 ~ 0.5 (완벽하게 분류되면 : 0, 완벽하게 섞이면 : 0.5)
- 지니 불순도가 낮은 속성으로 의사결정 트리 노드 결정
- 트리를 내려가면서 불순도가 낮아지면 좋다!

### **엔트로피(Entropy)**
- $\large Entropy = -\sum_{i=1}^m p_i\log_2 p_i$

- 엔트로피
  - 음성클래스비율 $\times \log_2$(음성클래스비율)
  - 양성클래스비율 $\times \log_2$(양성클래스비율)
- 엔트로피 값 : 0 ~ 1 (완벽하게 분류되면 : 0, 완벽하게 섞이면 : 1)

### **정보 이득 (Information Gain)**
- $\large Gain(T, X) = Entropy(T) - Entropy(T, X)$
- 정보 이득이 크다 = 어떤 속성으로 분할할 때 불순도가 줄어든다.
- 모든 속성에 대해 분할한 후 정보 이득 계산
- 정보 이득이 가장 큰 속성부터 분할

### **가지치기 (튜닝)**
- 가지치기를 하지 않으면 Overfitting 발생 가능성 매우 높음
- 여러 하이퍼파라미터 값을 조정해 가지치기 할 수 있음
  - **max_depth**, min_samples_leaf, min_samples_split, max_feature, max_leaf_node 등
- 가장 적절한 하이퍼파라미터 값을 찾도록 노력해야 함

### **결정트리 시각화**
- 여러 시각화 방법 중 Graphvis 패키지 사용
- 사전에 Graphvis 패키지 설치 및 운영체제 환경 설정이 진행 되어야 한다.
- 코드는 실습 코드 확인



## Logistic Regression
- 학습을 통해 선형 회귀선을 찾는 것이 아니라, 로지스틱 함수에 의해 반환되는 값을 확률로 간주하여 그 확률에 따라 분류를 결정함
- <a href='https://github.com/Jihwan98/aivle_school/blob/main/2023.02.20_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/%EB%B0%B0%EC%9A%B0%EA%B8%B0/ML03_04_%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%ED%9A%8C%EA%B7%80(Iris).ipynb' target='_blank'>Logistic Regression 실습코드(iris)</a>

### 로지스틱 함수

$\large p = \frac{1}{1 + e^{-f(x)}}$

- 시그모이드(Sigmoid) 함수라고도 부름
- 확률 값 p는 선형 판별식 값이 커지면 1, 작아지면 0에 가까운 값이 됨
- $(-\infty, \infty)$ 범위를 갖는 선형 판별식 결과로 $(0, 1)$ 범위의 확률 값을 얻게 됨
- 확률 값 0.5를 기준으로 이진 분류를 수행할 수 있게 됨
> x 데이터가 주어졌을 때 확률을 예측하는 로지스틱 회귀분석은 학습 데이터를 잘 설명하는 선형 판별식의 기울기(a)와 절편(b)를 찾는 문제


## SVM (Support Vector Machine)
- 분류를 위한 기준선, 즉 결정 경계선(Decision Boundary)을 찾는 알고리즘
- SVM 성능을 높이기 위해 <mark>정규화 작업</mark>이 필요
- 분류 문제와 회귀 문제 모두에 사용 사능(SVC, SVR)
- **결정 경계 (Decision Boundary)** : 서로 다른 분륫값을 결정하는 경계
- **서포트 벡터(Support Vector)** : 결정 경계선과 가장 가까운 데이터 포인트
- **마진(Margin)** :
    - 서포트 벡터와 결정 경계 사이의 거리
    - <u>마진을 최대로 하는 결정 경계를 찾는 것</u>이 SVM의 목표
    - 마진이 클수록 새로운 데이터에 대해 안정적으로 분류할 가능성이 높음
- <a href='https://github.com/Jihwan98/aivle_school/blob/main/2023.02.20_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/%EB%B0%B0%EC%9A%B0%EA%B8%B0/REF02_SVM%EC%9D%B4%ED%95%B4.ipynb' target='_blank'>SVM 이해 실습코드</a>
- <a href='https://github.com/Jihwan98/aivle_school/blob/main/2023.02.20_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/%EB%B0%B0%EC%9A%B0%EA%B8%B0/ML03_05_SVM(Admission)%20.ipynb' target='_blank'>SVM 실습코드(Admission)</a>

![svm](https://user-images.githubusercontent.com/76936390/221588688-86888c1c-f5b0-421f-bcbe-1307d6f658d5.png)

### **비용 (C)**
- 약간의 오류를 허용하기 위해 비용(C)이라는 변수를 사용
- 비용을 낮게 잡으면 (이상치들이 있을 가능성을 크게 봄)
    - 마진을 높이고 에러를 증가시키는 결정 경계선을 만듦 (Soft Margin) → 과소적합 위험
- 비용을 높게 잡으면 (이상치들이 있을 가능성을 작게 봄)
    - 마진은 낮추고 에러를 감소시키는 결정 경계선을 만듦 (Hard Margin) → 과대적합 위험
- SVM에서 적절한 비용 값을 찾는 과정이 매우 중요

### **커널트릭**
- 직선으로 분류할 수 없는 경우 **커널 트릭**을 통해 분류
- 저차원 데이터를 고차원 데이터로 옮긴 것과 같은 효과를 줘서 결정 경계선을 찾음
- rbf 커널(기본값), Poly, Linear
- rbf와 poly의 경우 gamma 설정이 필요

### **감마 (gamma)**
- 모델이 생성하는 경계가 복잡해지는 정도
- 값이 클 수록 경계가 복잡해지고, 값이 낮을 수록 경계가 단순해짐