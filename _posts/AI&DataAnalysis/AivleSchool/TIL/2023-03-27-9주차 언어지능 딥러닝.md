---
title:  "[KT Aivle 3기 AI] 9주차. 언어지능 딥러닝"
author: JIHWAN PARK
categories: [AI & 데이터분석, AIVLE SCHOOL]
tag: [AIVLE SCHOOL, Deep Learning]
math: true
date: 2023-03-27 21:45:42 +0900
last_modified_at: 2023-03-28 17:40:22 +0900
---
> KT Aivle School 3기 AI 9주차. 언어지능 딥러닝
> - 강사 : 김정훈 강사님
> - 주제 : 언어지능 딥러닝
> - 내용 :
>   - RNN(Recurrent Neural Network)에 대한 직관적인 이해 및 실습
>   - LSTM(Long Short Term Memory)과 GRU(Gated Recurrent Unit)에 대한 직관적인 이해 및 실습
>   - Bidirectional Layer와 Conv1D에 대한 직관적인 이해 및 실습
>   - 자연어 처리 방법에 대한 이해 및 실습
>   - Text Classification과 Text Generation에 대한 직관적인 이해 및 실습
{: .prompt-info}

<a href='https://github.com/Jihwan98/aivle_school/tree/main/2023.03.27_%EC%96%B8%EC%96%B4%EC%A7%80%EB%8A%A5%20%EB%94%A5%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C' target='_blank'>[실습 진행한 코드!]</a>

## 💡 38일차 TIL (03/27[월])

- <a href='https://github.com/Jihwan98/aivle_school/tree/main/2023.03.27_%EC%96%B8%EC%96%B4%EC%A7%80%EB%8A%A5%20%EB%94%A5%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/01_Sequence_Models' target='_blank'>[실습 진행한 코드!]</a>

**🌟 인공지능 개발자로서 꼭 필요한 능력!**
- 딥러닝의 구조를 기능적으로 해석하고 이해할 줄 알아야 한다!
- 결국 구조를 이해하고 파악하고 있다면, 어떤 언어나 라이브러리 등 문법만 간단히 익히면 사용할 수 있다!


### ✅ RNN(Recurrent Neural Network)
- RNN(Recurrent Neural Network)에 대해서 이해하고 실습해보았다.
- RNN은 시계열 데이터를(순서가 있는 데이터) 잘 학습하던 최초의 딥러닝 모델이다!

**✔️ RNN의 핵심!**

- <mark>과거의 (정돈된) 정보와 현재의 (raw) 정보를 동시에 고려하여, 모델의 목적에 맞는 Representation을 하여 새로운 특징을 제작해낸다.</mark>
- **위의 과정을 '일관된 규칙으로 반복'할 수 있다.**
- **시점이 달라져도, 정보를 추출하는 규칙은 동일하다 → 시점이 달라도, 중요하게 판단해야할 정보만을 추출한다. (같은 weight를 사용한다!)**

<details>
<summary>✨ <strong>+ RNN의 특징들</strong></summary>
<div markdown="1">

- RNN은 초기 시점의 정보를 굉장히 빠르게 잊어버린다.
    - 최근 정보를 더 잘 기억한다.
- tanh를 믿고 사용하자. 그렇지 않으면 Gradient가 난리난다.
- RNN Layer를 Hidden Layer라고 하는데, 시점별 정보를 언급하고 싶을 때는 'Hidden State'라는 표현을 사용한다. → t 시점의 Hidden State : $H^t$
- loss를 mae로 사용하는 경우, '급격한 학습을 막는데' 종종 사용이 된다.

- `return_sequences` 옵션에 대해서..
    - RNN 다음 레이어는 '모든 시점의 정보'를 전부 고려하도록 해라
        - ⇒ `return_sequences=True` : 모든 시점에서 제작한 특징값들을 다음 레이어로 다 넘겨라.
    - RNN이 제작한 정보 중, ‘맨 마지막 시점만’(최신 정보를 더 중하게) 다음 레이어가 고려하도록 하고 싶다.
        - ⇒ `return_sequences=False` : 맨 마지막 시점의 특징값들만 다음 레이어로 넘겨라.


</div>
</details>

<br>

### ✅ 기타

**✔️ 시계열 데이터를 바라볼 때**

1. Trend 확인
2. Cycle 확인

**✔️ 데이터 종류별 shape**

- Tabular.shape : [n rows, n cols]
- Image.shape : [#, H, W, C]
- Sequence.shape : [#, timestepsize, features]

---

## 💡 39일차 TIL (03/28[화])

- <a href='https://github.com/Jihwan98/aivle_school/tree/main/2023.03.27_%EC%96%B8%EC%96%B4%EC%A7%80%EB%8A%A5%20%EB%94%A5%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/01_Sequence_Models' target='_blank'>[실습 진행한 코드!]</a>

- 오늘은 어제 배운 RNN에 대한 이해를 바탕으로 <mark>LSTM과 GRU에 대한 직관적인 이해</mark>를 했다.

### ✅ LSTM (Long Short Term Memory)
- RNN의 금붕어 문제 일부 해결 
- LSTM은 RNN을 계승한 모델!
- LSTM = RNN의 구조적 특징 + Memory + Memory Managing
- <mark>LSTM = RNN의 구조적인 특징을 전부 가지고 있으면서 + '기억을 생성하고, 유지하고, 보수하는' 장치가 달려있다.</mark>
- 눈여겨 볼 점 : Gating 기법 (Sigmoid)
- 가중치 개수는 **RNN의 4배**
- LSTM은 똑똑하고 무거운 금붕어다.

<details>
<summary>✨ <strong>+ LSTM의 특징들</strong></summary>
<div markdown="1">

- Cell State 와 Hidden State가 존재
- **✔️ Cell State는  Memory**역할을 함
- **Forget Gate** : Sigmoid를 통과하여 기억에 대한 강도를 조절
- **Input Gate** : Cell State에 기록할 가지가 있는지 조절
- **Update Cell State** : Old + Net
    - 현 시점의 기억 = 과거의 기억 * Forget Gate + 기억의 후보 * Input Gate
    - $C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t$
- **New Hidden State** : from Memory & from Now
    - $h_t = o_t \cdot tanh(C_t)$
    - $o_t = \sigma(W_0[h_{t-1}, x_t] + b_0)$
    - $tanh(C_t)$ : 통합된 기억을 -1 ~ 1로 squeeze
        1. Normalize : 규격을 맞춘다.
        2. Hidden State 규격 (RNN)

</div>
</details>

<br>

### ✅ GRU (Gated Recurrent Unit)
- Gating 기법을 정돈 함
- <mark>Cell State (Memory) + Hidden State를 통합 ⇒ Hidden State 자체가 Memory의 역할</mark>
- 과거의 기억과 현재의 기억을 일정 비율로 가져온다. (Update Gate ($z_t$))
- 가중치 개수는 **RNN의 3배**
- LSTM 대비 모델의 무게는 75%이나, LSTM과 성능이 비슷하다!

<details>
<summary>✨ <strong>+ GRU의 특징들</strong></summary>
<div markdown="1">

- **Reset Gate** : LSTM의 Forget Gate와 비슷
    - $r_t = \sigma(W_z \cdot [h_{t-1}, x_t])$
- **Update Gate** : 과거의 기억과 현재의 기억을 일정 비율로 가져온다. 비율 ($z_t$) 생성.
    - $z_t = \sigma(W_z \cdot [h_{t-1}, x_t])$
- **Current Memory** : LSTM의 Input Gate와 비슷
    - $\tilde{h_t} = tanh(W \cdot [r_t * h_{t-1}, x_t])$
- **New Hidden State** : from Old & from Now
    - 과거의 기억과 현재의 기억을 일정 비율로 가져온다.
    - $h_t = (1 - z_t) * h_{t-1} + z_t * \tilde{h_t}$


</div>
</details>

<br>

### ✅ RNN, LSTM, GRU 비교 표

||RNN|LSTM|GRU|
|---|---|---|---|
|가중치 개수|한 세트 당|RNN의 4배|RNN의 3배|
|Memory|X|O|O|


### ✅ 기타

**✔️ 기호에 대한 의미**

- $\bar{y}$ : y bar 평균
- $\hat{y}$ : y hat 예측/추론
- $\tilde{y}$ : y tilde y가 될 수 있는 후보 (candidate)



**✔️ 각 task별 특징에 대해 이해해야 한다.**

언어에 대한 예시로는 한국어와 영어의 특징을 들 수 있다.

- 한국어는 조사가 있기 때문에 순서가 상관이 없다.
    - ex) 너는 이거 못하지, 너는 못하지 이거, 이거 너는 못하지, 못하지 너는 이거, ...

- 영어는 위치가 품사를 정한다. 
    - ex) I will skin you. I water plant.

---


## 💡 40일차 TIL (03/29[수])
> 오늘은 수업 외적으로 강사님께서 우리가 나아가야할 방향성에(취업을 위한) 대해서 많은 말씀을 해주셨다.. 정말 감사드립니다!!

- <a href='https://github.com/Jihwan98/aivle_school/tree/main/2023.03.27_%EC%96%B8%EC%96%B4%EC%A7%80%EB%8A%A5%20%EB%94%A5%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/01_Sequence_Models' target='_blank'>[실습 진행한 코드!]</a>

- 오늘은 **Bidirectional Layer**와 **Conv1D**에 대한 직관적인 이해를 하고, 실습해보았다.

### ✅ Bidirectional Layer
- 양방향으로 살펴보는 구조를 가지고 있다.
    - 정방향 : 최신 정보를 '과거의 맥락'을 고려하여 반영
    - 역방향 : 첫 시점의 정보를 '미래의 맥락'을 고려하여 반영
    - ⇒ 모든 시점의 맥락을 고려하여 정보를 추출할 수 있음
- ~~모델 구조상 Flatten해서 모든 시점을 살펴보는 것이 의미가 있을 것?~~

### ✅ Conv1D
- LSTM은 모든 시점이 고려됨
- Conv1D는 관찰하는 시점 외에는 관심이 없다.
- 시점을 축소하는 역할을 가짐
- 그래서 먼저 Conv1D로 시점을 축소한 다음, LSTM이나 GRU, Bidirectional Layer 등등에 넣어줌으로써, 해당 모델들의 단점이었던 오래된 정보들에 대해 까먹게되는 현상을 보완할 수 있다.

---


## 💡 41일차 TIL (03/30[목])

- <a href='https://github.com/Jihwan98/aivle_school/tree/main/2023.03.27_%EC%96%B8%EC%96%B4%EC%A7%80%EB%8A%A5%20%EB%94%A5%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/02_Text_classification' target='_blank'>[Text Classification 실습 진행한 코드!]</a>
- <a href='https://github.com/Jihwan98/aivle_school/tree/main/2023.03.27_%EC%96%B8%EC%96%B4%EC%A7%80%EB%8A%A5%20%EB%94%A5%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/03_Text_generation' target='_blank'>[Text Generation 실습 진행한 코드!]</a>

- 자연어 처리 방법에 대한 이해 및 실습을 진행
- Text Classification과 Text Generation에 대한 직관적인 이해 및 실습을 진행

<mark>✔️ 자연어 처리의 뼈대</mark>
: 
1. <mark>Tokenize</mark>
2. <mark>Embedding</mark>
3. <mark>Modeling</mark>

### ✅ Text Classification & Embedding
✔️ 예전 방법
: 
- Bag of Words : index를 지정 (+ 등장 횟수를 count 해서 많은 순)
- Tf idf : ~
- ⇒ 예전 방법의 의미!
    1. <mark>어떻게든 '숫자'로 변환했다. → 계산이 가능</mark>
    2. 너무 sparse 하다 (죄다 0 투성이)

✔️ Word Embedding
: 예전 방법은 사람 관점에서 전처리 했다. 그래서 조금 더 기계 관점으로 전처리 한 방법.
1. Tokenize = (word) token : idx (`Tokenize()`)
2. Text Sequence → Index Sequence (`tk.texts_to_sequences()`)
3. 문장 길이를 통일 (`pad_sequences()`)
    - padding, truncate
    - 보편적으로는 앞에 붙이고, 앞을 살린다(뒤를 자른다)
    - > `tensorflow.keras.prepocessing.sequence.pad_sequences`의 truncate default 값은 앞을 자르는 것이다 (`truncate='pre'`)

✔️ **Embedding Layer**에서 학습이 될 숫자로 바꿔준다(Vector) → 이 값도 학습을 통해 업데이트
: 
1. 성능에 도움? : 긍/부 분류하에 의미 있음
2. 의미분석? : 가능성은 열림 (어학적 의미는 X)


✨ Word Embedding 에는
: 
- word2vec
- Doc2vec
- Elmo
- FastText
