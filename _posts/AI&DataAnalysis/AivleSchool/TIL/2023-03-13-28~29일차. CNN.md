---
title:  "[KT Aivle 3기 AI] 28~29일차. CNN(Convolutional Neural Network)"
author: JIHWAN PARK
categories: [AI & 데이터분석, AIVLE SCHOOL]
tag: [AIVLE SCHOOL, Machine Learning, Deep Learning, CNN]
math: true
date: 2023-03-13 18:00:00 +0900
last_modified_at: 2023-03-14 18:21:00 +0900
---
> KT Aivle School 3기 AI 28~29일차 
> - 강사 : 김건영 강사님
> - 주제 : CNN
> - 내용 :
>   - CNN에 대해서 배움
>   - stride, zero-padding, Max Pooling 등에 대해 배움
>   - Image Data Augmentation 에 대해서 배우고 실습
>   - 코드로 구현
{: .prompt-info}

## 💡 28일차. TIL
<details>
<summary>28일차(3/13)</summary>
<div markdown="1">
오늘은 CNN에 대해서 배웠다. 이미 학부때 배웠던 내용이라 다시 기억을 상기시키며 들었다. 

CNN이 나오게 된 계기 : 이미지의 위치정보를 없애지 않고 처리하기 위함

CNN 연산에는 kernel_size와 stride가 영향을 미치며, 기본적으로 사이즈가 작아지고 합성곱의 불균형이 생긴다.

CNN 연산 결과 크기 공식 : `output_size = (N - F) / stride + 1`

사이즈가 작아지고 합성곱의 불균형을 해결하기 위해 zero-padding이라는 기법이 생겼다.

그리고, 연산량을 줄이기 위해 Max Pooling이라는 기법이 생겼는데, Max Pooling은 filter_size에서 가장 큰 값을 가져오는 것이다.

~~`relu` 함수를 주로 쓰는 상황에서 Max Pooling이 가장 알맞다고 볼 수 있다?~~

그리고, Batch Normalization과 Dropout에 대해 간단하게 배우고 사용했는데, 둘 다 모델을 Robust하게 만들어주는 기능이다.

Batch Normalization은 mini batch를 정규화 시켜주고, Dropout은 지정한 비율로 랜덤하게 노드를 죽이게 되어 초기 weight 값에 덜 민감하게 만들어 준다.

y가 범주형인데 numerical으로 있는 경우 (ex: y = [1, 2, 3, 4, 5, ...]) 원래는 `to_categorical`으로 전처리를 해줘야 하지만,

전처리 없이 `loss = keras.losses.sparse_categorical_crossentropy`을 사용하면 학습 가능하다.


<a href='https://github.com/Jihwan98/aivle_school/tree/main/2023.03.13.CNN_2.0ver/1_My_First_CNN' target='_blank'>[CNN 관련 코드]</a>

CNN Modeling 예시 코드

```python
# 1. 세션 클리어
keras.backend.clear_session()

# 2. 모델 연결
il = Input(shape=(28, 28, 1))
cl = Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu')(il)
bl = BatchNormalization()(cl)
cl = Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu')(bl)
bl = BatchNormalization()(cl)
pl = MaxPool2D(pool_size=(2, 2))(bl)
dl = Dropout(0.25)(pl)

cl = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu')(il)
bl = BatchNormalization()(cl)
cl = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu')(bl)
bl = BatchNormalization()(cl)
pl = MaxPool2D(pool_size=(2, 2))(bl)
dl = Dropout(0.25)(pl)

fl = Flatten()(dl)
dl = Dense(512, activation='relu')(fl)
bl = BatchNormalization()(dl)
ol = Dense(10, activation='softmax')(bl)

model = keras.models.Model(il, ol)

# 3. 모델 컴파일
model.compile(loss=keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'], optimizer='adam')

# 4. 요약
model.summary()
```
</div>
</details>


## 💡 29일차. TIL
<details>
<summary>29일차(3/14)</summary>
<div markdown="1">
오늘은 어제에 이어 CNN 모델을 만들고 학습을 진행했다.

그리고 Image Data Augmentation에 대해 간략히 배우고 실습을 진행했다.

실제 현실에서는 충분한 Data가 있지 않으므로, 부족한 데이터를 채워주기 위해 기존의 데이터를 조금 변형 시켜 데이터를 늘리는 방법이 Data Augmentation이다.

이를 keras의 ImageDataGenerator를 통해 진행할 수 있다.

<a href='https://github.com/Jihwan98/aivle_school/tree/main/2023.03.13.CNN_2.0ver/2_Data_augmentation_and_more' target='_blank'>[Augmentaion 관련 코드]</a>

</div>
</details>