---
title: '[KT Aivle 3기 AI] 17일차. 머신러닝 (4) Hyperparameter 튜닝'
author: JIHWAN PARK
categories:
  - AI & 데이터분석
  - AIVLE SCHOOL
tag:
  - AIVLE SCHOOL
  - Machine Learning
  - Python
math: true
date: '2023-02-25 19:00:00 +0900'
last_modified_at: '2023-03-03 18:00:00 +0900'
published: true
image: "https://github.com/Jihwan98/Jihwan98.github.io/assets/76936390/6be11e55-36a3-4a86-8e30-d8928f732a0c"
---
> KT Aivle School 3기 AI 17일차 
> - 강사 : 이장래 강사님
> - 주제 : 기본 알고리즘 중 Logistic Regression, SVM과 K-Fold Cross Validation, Hyperparameter 튜닝(Grid Search, Random Search)
> - 내용 :
>   - K-Fold Cross Validation 설명 및 실습
>   - Hyperparameter 튜닝 방법 설명 (Grid Search, Random Search)
>   - 모델의 복잡성, 클래스 불균형
{: .prompt-info}

# K-분할 교차 검증 (K-Fold Cross Validation)
- 모든 데이터가 평가에 한 번, 학습에 k-1번 사용
- k는 2 이상
- <a href='https://github.com/Jihwan98/aivle_school/blob/main/2023.02.20_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/%EB%B0%B0%EC%9A%B0%EA%B8%B0/ML04_01_K%EB%B6%84%ED%95%A0%EA%B5%90%EC%B0%A8%EA%B2%80%EC%A6%9D(Diabetes).ipynb' target='_blank'>K-Fold Cross Validation 실습코드(Diabetes)</a>

# Hyperparameter
- 알고리즘을 사용하여 모델링 할 때, 최적화하기 위해 조절할 수 있는 옵션
- 정답은 없음! 다양한 시도가 중요하다. (Grid Search, Random Search)

## 모델 별 Hyperparameter 예시
### **KNN**
- k 값 (n_neighbors)
- 거리 계산법 (metric)

### **Decision Tree**
- max_depth : 트리의 최대 깊이 제한
- min_samples_leaf : leaf가 되기 위한 최소한의 샘플 데이터 수 (default : 1)
- min_samples_split : 노드를 분할하기 위한 최소한의 샘플 데이터 수(default : 2)

## Grid Search & Random Search
- Grid Search : 설정한 범위의 모든 값에 대한 결과 확인
- Random Search : 설정한 범위에서 설정한 개수 만큼만 랜덤하게 뽑아서 결과 확인

- → Random Search로 구하고, 그 근처를 Grid Search로 진행 하는 방법
- → Grid Search로 진행 한 후 해당 범위의 중앙에 값이 나오면 쓰고~ 범위 끝 쪽 값으로 나오면 범위를 그 방향으로 늘려서 또 돌려보고~ 하는 방법
- → 최선의 파라미터를 찾도록 다양한 방법 활용

- grid search model 에서 `model.cv_results_` 속성에 성능 테스트와 관련된 많은 정보가 포함되어있다.
  - `model.cv_results_['mean_test_score']`: 테스트로 얻은 성능
  - `model.best_params_`: 최적의 파라미터
  - `model.best_score_`: 최고의 성능

- <a href='https://github.com/Jihwan98/aivle_school/blob/main/2023.02.20_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/%EB%B0%B0%EC%9A%B0%EA%B8%B0/ML04_03_GridSearch(%EA%B2%B0%EC%A0%95%ED%8A%B8%EB%A6%AC%2CBoston).ipynb' target='_blank'>Grid Search 실습코드(결정트리, Boston)</a>
- <a href='https://github.com/Jihwan98/aivle_school/blob/main/2023.02.20_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/%EC%9D%B5%ED%9E%88%EA%B8%B0/%EC%8B%A4%EC%8A%B504_06_GridSearch(%EA%B2%B0%EC%A0%95%ED%8A%B8%EB%A6%AC%2CMobile).ipynb' target='_blank'>Grid Search 실습코드(결정트리, Mobile)</a>

# 모델의 복잡성 
- 모델이 복잡하다는 것은
  - 학습 데이터에 너무 치중한 나머지 실제 평가시 성능이 좋지 않은 것을 의미
  - 즉, 과적합 위험이 있는 것을 의미
- Linear Regression : 독립 변수가 많을 수록
- KNN : k값, 즉 n_neighbors 값이 작을 수록
- Decision Tree : max_depth가 클 수록 복잡한 모델이 됨
- 결국 우리는 적절한 복잡성을 갖는 모델을 만들어야 함
- 너무 단순해도 안되고, 너무 복잡해도 안됨
- <a href='https://github.com/Jihwan98/aivle_school/blob/main/2023.02.20_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/%EB%B0%B0%EC%9A%B0%EA%B8%B0/REF12_KNN%EB%B3%B5%EC%9E%A1%EB%8F%84.ipynb' target='_blank'>KNN 복잡도 실습코드</a>

# 클래스 불균형
- 정답 클래스의 값이 불균형하게 있을 경우 (ex: 0 - 100개, 1 - 10개)
- 1에 대한 recall이 떨어짐. 우리는 1에 대한 recall을 올리고 싶다.
- under sampling : 개수가 많은 데이터를 개수가 적은 데이터의 수에 맞춤 (0 - 10개, 1 - 10개)
- over sampling : 개수가 적은 데이터를 개수가 많은 데이터의 수에 맞춤 (0 - 100개, 1 - 100개)
- class_weight = ‘balanced’ : 모델의 weight를 조절하여 맞춤
- <a href='https://github.com/Jihwan98/aivle_school/blob/main/2023.02.20_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/%EB%B0%B0%EC%9A%B0%EA%B8%B0/REF05_%ED%81%B4%EB%9E%98%EC%8A%A4%EB%B6%88%EA%B7%A0%ED%98%95(Attrition).ipynb' target='_blank'>클래스 불균형 관련 코드</a>
