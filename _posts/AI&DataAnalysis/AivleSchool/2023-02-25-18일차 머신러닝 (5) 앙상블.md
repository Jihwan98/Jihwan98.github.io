---
title:  "[KT Aivle 3기 AI] 18일차. 머신러닝 (5) 앙상블 알고리즘"
author: JIHWAN PARK
categories: [AI & 데이터분석, AIVLE SCHOOL]
tag: [AIVLE SCHOOL, Machine Learning, Python]
math: true
date: 2023-02-25 20:00:00 +0900
last_modified_at: 2023-03-03 18:00:00 +0900
---
> KT Aivle School 3기 AI 18일차 
> - 강사 : 이장래 강사님
> - 주제 : 앙상블 알고리즘
> - 내용 :
>   - 앙상블(Ensemble) 알고리즘 설명 및 실습
{: .prompt-info}

# 앙상블 (Ensemble)
- 통합은 힘이다.
- 여러개의 모델을 결합하여 훨씬 강력한 모델을 생성하는 기법
- 많은 기계학습 경쟁에서 상위 순위를 차지하고 있음
- <a href='https://github.com/Jihwan98/aivle_school/blob/main/2023.02.20_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/%EB%B0%B0%EC%9A%B0%EA%B8%B0/ML04_04_%EC%95%99%EC%83%81%EB%B8%94(Admission).ipynb' target='_blank'>앙상블 실습 코드(Admission)</a>

## 보팅 (Voting)
- 여러 개의 분류기가 투표를 통해 최종 예측 결과를 결정하는 방식
- 하드 보팅 (Hard Voting) : 다수 분류기가 예측한 값
- 소프트 보팅 (Soft Voting) : 모든 분류기가 예측한 값의 확률의 평균을 구해 높은 값

## 배깅 (Bagging)
- Bootstrap Aggregating의 약자
- 데이터로부터 부트스트랩 한 데이터로 모델을 학습 시킨 후, 학습된 모델의 결과를 집계하여 최종 결과를 얻는 방법
- <mark>같은 알고리즘을 사용</mark>

### **랜덤 포레스트 (Random Forest)**
- 배깅(Bagging)의 가장 대표적인 알고리즘
- 랜덤하게 데이터를 샘플링 (복원 추출), Feature를 랜덤하게 선정 → 각자 개별 학습
- 주요 하이퍼파라미터 : 
  - `n_estimators` : 만들어질 Decision Tree 개수 지정 (default : 100)
  - `max_depth` : 트리의 최대 깊이 (default : None)

## 부스팅 (Boosting)
- 같은 유형의 알고리즘 기반 분류기 여러 개에 대해 순차적으로 학습을 진행
- 이전 분류기 예측이 틀린 데이터에 대해서 올바르게 예측할 수 있도록 다음 분류기에게 가중치(Weight)를 부여하면서 학습과 예측을 진행
- 계속하여 분류기에게 가중치를 부스팅하며 학습을 진행해 부스팅 방식이라 함
- 대표적인 부스팅 방법 : XGBoost, LightGBM

**Gradient Boost**
- 오차를 찾는 모델을 계속 생성
- $y = m_1(x) + err_1(x), \quad\quad err_1(x) = m_2(x) + err_2(x)$
- $y = m_1(x) + m_2(x) + err_2(x), \quad err_2(x) = m_3(x) + err_3(x)$
- $\cdots$

**XGBoost(eXtreme Gradient Boosting)**
- GBM(Gradient Boost Machine) 알고리즘을 병렬 학습이 가능하도록 구현한 것이 XGBoost
- 회귀, 분류 문제를 모두 지원하며, 성능과 자원 효율이 좋아 많이 사용됨

**LightGBM**
- 조금 더 가벼운?(빠른?) GBM

## 스태킹 (Stacking)
- 여러 모델의 예측 값을 최종 모델의 학습 데이터로 사용하여 예측하는 방법
- ex) 
  - KNN, Logistic Regression, XGBoost 모델을 사용해 4종류 예측값을 구한 후
  - 이 예측 값을 최종 모델인 Randomforest 학습 데이터로 사용
- 현실 모델에서 많이 사용되지는 않음
- 기본 모델로 4개 이상 선택해야 좋은 결과를 기대할 수 있다.