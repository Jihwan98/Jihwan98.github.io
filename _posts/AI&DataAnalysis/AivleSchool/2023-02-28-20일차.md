---
title:  "[KT Aivle 3기 AI] 20일차. 딥러닝 (2)"
author: JIHWAN PARK
categories: [AI & 데이터분석, AIVLE SCHOOL]
tag: [AIVLE SCHOOL, Deep Learning]
math: true
date: 2023-02-28 18:00:00 +0900
last_modified_at: 2023-02-28 18:00:00 +0900
---
> KT Aivle School 3기 AI 20일차 
> - 강사 : 김건영 강사님
> - 주제 : 딥러닝 공부를 위한 기본 토대 쌓기
> - 내용 :
>   - 어제 공부한 Sequential API에서 히든 레이어를 추가하는 방법
>   - 히든 레이어와 노드는 무슨 역할을 하는가
>   - MNIST 실습 ()
{: .prompt-info}

## 히든 레이어
- 입력층과 출력층 사이에 있는 layer를 말한다.
- `Dense(n, activation='relu')`로 구현을 할 수 있다.
- n개 만큼의 노드가 생성된다고 할 수 있다?

### **히든 레이어와 노드는 무슨 역할을 하는가?**
<mark>연결된 것으로부터 기존에 없던 새로운 Feature를 추출/재표현 하는 것 = <strong>Feature Representation</strong></mark>

1. layer가 쌓일 수록 Low Level → High Level의 기존에 없던 특징을 새롭게 뽑아내는 것
2. 노드의 수는 해당 Level에서 연결된 것으로부터 새롭게 추출하려는 Feature의 수

학습이 잘 되었다(새로운 레이어로 성능이 좋아졌다)라는 말은 **연결된 것으로부터 기존에 없던 새로운 Feature를 잘 추출해냈다 = Feature Representation(Feature Learning) : 딥러닝 핵심 개념 중 하나!**

> **Machine Learning vs Deep Learning**
> - Machine Learning : Feature Extraction을 사람이 직접 해줘야 함
> - Deep Learning : Feature Extraction을 컴퓨터가 찾아냄
>> 다양한 형태의 데이터(이미지, 텍스트, 음성, ...)를 다루려면 Deep Learning을 공부해야 한다.

## MNIST
- <a href='https://github.com/Jihwan98/aivle_school/blob/main/2023.02.27_%EB%94%A5%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/2_2_ANN_MNIST.ipynb' target='_blank'>MNIST ANN 코드실습</a>
- 딥러닝의 'Hello Wolrd'인 MNIST 데이터로 실습을 진행
- 전처리로 Min-Max Scaling : 픽셀 값 [0 ~ 255] → [0~1]
- Early Stopping : epoch 제한 해제
  - monitor : 관측 대상 (default : val_loss)
  - patience : 몇 번 참을래?
  - min_delta : Threshold
  - restore_best_weights : 최적의 가중치로 반영해라 (default : False)
- .fit(validation_split = 0.2) : training set에서부터 알아서 분배해서 가져간다.

**Early Stopping과 fit 부분만 간단하게 살펴보기**

```python
from tensorflow.keras.callbacks import EarlyStopping

es = EarlyStopping(monitor='val_loss',            # 관측 대상
                   min_delta=0,                   # 학습 성능 Threshold
                   patience=5,                    # 성능 개선되지 않더라도 몇 번 참을래?
                   verbose=1,                     
                   restore_best_weights=True)     # 가장 성능이 좋았던 epochs의 가중치를 쓸래 (Default=False)

model.fit(train_x, train_y, validation_split=0.2, 
          callbacks=[es], verbose=1, epochs=50)
```