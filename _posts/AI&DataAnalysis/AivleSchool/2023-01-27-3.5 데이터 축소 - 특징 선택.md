---
title:  "[사전학습] 3.5 데이터 축소 - 특징 선택"
author: JIHWAN PARK
categories: [AI & 데이터분석, AIVLE SCHOOL]
tag: [사전학습, AIVLE SCHOOL, Python, Jupyter Notebook, Numpy, Pandas]

date: 2023-01-27 22:00:00 +0900
last_modified_at: 2023-01-27 22:00:00 +0900
---


# 3. 데이터 전처리 이해와 실무
## 3.1 데이터 축소 : 특징 선택
### 특징 선택 (Feature Selection)
가장 좋은 성능을 보여줄 수 있는 데이터의 부분 집합(Subset)을 찾아내는 방법  
=> 모델 생성에 밀접한 **데이터의 부분 집합을 선택**하여 **연산 효율성 및 모델 성능을 확보**
#### 목적 및 필요성
연산 효율성
- 특징 생성과는 다르게 원 데이터 공간 내 유의미한 특징을 선택하는 기법으로 연산 효율 및 적절한 특징을 찾기 위해 수행
- **원본 데이터에서 가장 유용한 특징만을 선택**하여 간단한 모델 구성 및 성능을 확보하고자 하는 것이 주요 목적
### 특징 선택 방안
#### 1. 필터(Filter)
- 특징들에 대한 **통계적 점수**를 부여하여 순위를 매기고 선택하는 방법론
- **실행 속도가 빠르다**는 측면에서 시간 및 비용 측면의 장점을 보임
##### 1) 카이제곱 필터 (Chi-square filter)
- **범주형인 독립 및 종속 변수 간의 유의미성**을 도출하기 위한 통계적 방안
- 연소곃ㅇ 변수를 이산화(범주)를 하여 활용 가능
##### 2) 상관관계 필터(Correlation filter)
- **연속형인 독립 및 종속변수 간 유의미성**을 도출하기 위한 통계적 방안
- 보통 임계치(threshold) 설정하여 변수 선택

#### 2. 래퍼(Wrapper)
- **특징들의 조합**을 지도학습 기반 알고리즘에 **반복적으로 적용하여 특징을 선택**하는 방법론
- 최적의 데이터 조합을 찾기 때문에 성능 관점 상 유용하나 **시간과 비용 크게 발생**
- 원본 데이터 내 변수 간 조합을 탐색하여 특징 선택
- 반복적 특징 조합 탐색
    - 원본 데이터 셋 내 변수들의 **다양한 조합을 모델에 적용**하는 방식
    - 최적의 부분 데이터 집합(Subset)을 도출하는 방법론
    - 대표적 방식으로 **재귀적 특성 제거(Recursive Feature Elimination) 존재**


#### 3. 임베디드(Embedded)
- **모델 정확도에 기여하는 특징들을 선택**하는 방법으로 Filter와 Wrpper의 장점을 결합한 방법
- 모델의 학습 및 생성과정에서 최적의 특징을 선택하는 방법
- 모델을 학습하여 정확도에 기여하는 특징을 선택하는 방안
- 모델 기반 특징 선택
    - 알고리즘 내 자체 내장 함수로 **특징을 선택**하는 방식으로, **모델 성능에 기여하는 특징을 도출**
    - 모든 조합을 고려하고 결과를 도출하는 Wrapper 방식과 달리 **학습과정에서 최적화된 변수를 선택**
    - 트리 계열 모델 기반의 특징 선택이 대표적(랜덤포레스트 기반 Featuer Importance 기반)

#### 4. 특징 선택 알고리즘
랜덤포레스트 모형 기반의 알고리즘
##### 보루타 알고리즘(Boruta Algorithm)
- Boruta Algorithm은 기존 데이터 임의로 복제하여 **랜덤 변수(shadow) 생성**하고 원 자료와 결합하여 랜덤포레스트 모형에 적용
- **shadow 보다 중요도가 낮을 경우** 중요하지 않은 변수로 판단 후 제거

## 실습


```python
import numpy as np
import pandas as pd
```


```python
cancer = pd.read_csv("./data/wdbc.data", header=None)

# 데이터 컬럼명 지정
cancer.columns = [
    "id", "diagnosis", "radius_mean", "texture_mean", "perimeter_mean", "area_mean", "smoothness_mean", "compactness_mean", 
    "concavity_mean", "concave_points_mean", "symmetry_mean", "fractal_dimension_mean", "radius_se", "texture_se",
    "perimeter_se", "texture_worst", "smoothness_se", "compactness_se", "concavity_se", "concave_points_se", "symmetry_se",
    "fractal_dimension_se", "radius_worst", "texture_worst", "perimeter_worst", "area_worst", "smoothness_worst", "compactness_worst", 
    "concavity_worst", "concave_points_worst", "symmetry_worst", "fractal_dimension_worst"
]

# ID를 Index화
cancer = cancer.set_index('id')
cancer
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>diagnosis</th>
      <th>radius_mean</th>
      <th>texture_mean</th>
      <th>perimeter_mean</th>
      <th>area_mean</th>
      <th>smoothness_mean</th>
      <th>compactness_mean</th>
      <th>concavity_mean</th>
      <th>concave_points_mean</th>
      <th>symmetry_mean</th>
      <th>...</th>
      <th>radius_worst</th>
      <th>texture_worst</th>
      <th>perimeter_worst</th>
      <th>area_worst</th>
      <th>smoothness_worst</th>
      <th>compactness_worst</th>
      <th>concavity_worst</th>
      <th>concave_points_worst</th>
      <th>symmetry_worst</th>
      <th>fractal_dimension_worst</th>
    </tr>
    <tr>
      <th>id</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>842302</th>
      <td>M</td>
      <td>17.99</td>
      <td>10.38</td>
      <td>122.80</td>
      <td>1001.0</td>
      <td>0.11840</td>
      <td>0.27760</td>
      <td>0.30010</td>
      <td>0.14710</td>
      <td>0.2419</td>
      <td>...</td>
      <td>25.380</td>
      <td>17.33</td>
      <td>184.60</td>
      <td>2019.0</td>
      <td>0.16220</td>
      <td>0.66560</td>
      <td>0.7119</td>
      <td>0.2654</td>
      <td>0.4601</td>
      <td>0.11890</td>
    </tr>
    <tr>
      <th>842517</th>
      <td>M</td>
      <td>20.57</td>
      <td>17.77</td>
      <td>132.90</td>
      <td>1326.0</td>
      <td>0.08474</td>
      <td>0.07864</td>
      <td>0.08690</td>
      <td>0.07017</td>
      <td>0.1812</td>
      <td>...</td>
      <td>24.990</td>
      <td>23.41</td>
      <td>158.80</td>
      <td>1956.0</td>
      <td>0.12380</td>
      <td>0.18660</td>
      <td>0.2416</td>
      <td>0.1860</td>
      <td>0.2750</td>
      <td>0.08902</td>
    </tr>
    <tr>
      <th>84300903</th>
      <td>M</td>
      <td>19.69</td>
      <td>21.25</td>
      <td>130.00</td>
      <td>1203.0</td>
      <td>0.10960</td>
      <td>0.15990</td>
      <td>0.19740</td>
      <td>0.12790</td>
      <td>0.2069</td>
      <td>...</td>
      <td>23.570</td>
      <td>25.53</td>
      <td>152.50</td>
      <td>1709.0</td>
      <td>0.14440</td>
      <td>0.42450</td>
      <td>0.4504</td>
      <td>0.2430</td>
      <td>0.3613</td>
      <td>0.08758</td>
    </tr>
    <tr>
      <th>84348301</th>
      <td>M</td>
      <td>11.42</td>
      <td>20.38</td>
      <td>77.58</td>
      <td>386.1</td>
      <td>0.14250</td>
      <td>0.28390</td>
      <td>0.24140</td>
      <td>0.10520</td>
      <td>0.2597</td>
      <td>...</td>
      <td>14.910</td>
      <td>26.50</td>
      <td>98.87</td>
      <td>567.7</td>
      <td>0.20980</td>
      <td>0.86630</td>
      <td>0.6869</td>
      <td>0.2575</td>
      <td>0.6638</td>
      <td>0.17300</td>
    </tr>
    <tr>
      <th>84358402</th>
      <td>M</td>
      <td>20.29</td>
      <td>14.34</td>
      <td>135.10</td>
      <td>1297.0</td>
      <td>0.10030</td>
      <td>0.13280</td>
      <td>0.19800</td>
      <td>0.10430</td>
      <td>0.1809</td>
      <td>...</td>
      <td>22.540</td>
      <td>16.67</td>
      <td>152.20</td>
      <td>1575.0</td>
      <td>0.13740</td>
      <td>0.20500</td>
      <td>0.4000</td>
      <td>0.1625</td>
      <td>0.2364</td>
      <td>0.07678</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>926424</th>
      <td>M</td>
      <td>21.56</td>
      <td>22.39</td>
      <td>142.00</td>
      <td>1479.0</td>
      <td>0.11100</td>
      <td>0.11590</td>
      <td>0.24390</td>
      <td>0.13890</td>
      <td>0.1726</td>
      <td>...</td>
      <td>25.450</td>
      <td>26.40</td>
      <td>166.10</td>
      <td>2027.0</td>
      <td>0.14100</td>
      <td>0.21130</td>
      <td>0.4107</td>
      <td>0.2216</td>
      <td>0.2060</td>
      <td>0.07115</td>
    </tr>
    <tr>
      <th>926682</th>
      <td>M</td>
      <td>20.13</td>
      <td>28.25</td>
      <td>131.20</td>
      <td>1261.0</td>
      <td>0.09780</td>
      <td>0.10340</td>
      <td>0.14400</td>
      <td>0.09791</td>
      <td>0.1752</td>
      <td>...</td>
      <td>23.690</td>
      <td>38.25</td>
      <td>155.00</td>
      <td>1731.0</td>
      <td>0.11660</td>
      <td>0.19220</td>
      <td>0.3215</td>
      <td>0.1628</td>
      <td>0.2572</td>
      <td>0.06637</td>
    </tr>
    <tr>
      <th>926954</th>
      <td>M</td>
      <td>16.60</td>
      <td>28.08</td>
      <td>108.30</td>
      <td>858.1</td>
      <td>0.08455</td>
      <td>0.10230</td>
      <td>0.09251</td>
      <td>0.05302</td>
      <td>0.1590</td>
      <td>...</td>
      <td>18.980</td>
      <td>34.12</td>
      <td>126.70</td>
      <td>1124.0</td>
      <td>0.11390</td>
      <td>0.30940</td>
      <td>0.3403</td>
      <td>0.1418</td>
      <td>0.2218</td>
      <td>0.07820</td>
    </tr>
    <tr>
      <th>927241</th>
      <td>M</td>
      <td>20.60</td>
      <td>29.33</td>
      <td>140.10</td>
      <td>1265.0</td>
      <td>0.11780</td>
      <td>0.27700</td>
      <td>0.35140</td>
      <td>0.15200</td>
      <td>0.2397</td>
      <td>...</td>
      <td>25.740</td>
      <td>39.42</td>
      <td>184.60</td>
      <td>1821.0</td>
      <td>0.16500</td>
      <td>0.86810</td>
      <td>0.9387</td>
      <td>0.2650</td>
      <td>0.4087</td>
      <td>0.12400</td>
    </tr>
    <tr>
      <th>92751</th>
      <td>B</td>
      <td>7.76</td>
      <td>24.54</td>
      <td>47.92</td>
      <td>181.0</td>
      <td>0.05263</td>
      <td>0.04362</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.1587</td>
      <td>...</td>
      <td>9.456</td>
      <td>30.37</td>
      <td>59.16</td>
      <td>268.6</td>
      <td>0.08996</td>
      <td>0.06444</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>0.2871</td>
      <td>0.07039</td>
    </tr>
  </tbody>
</table>
<p>569 rows × 31 columns</p>
</div>




```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(cancer.drop(['diagnosis'], axis=1), cancer[['diagnosis']], random_state=1)
print(X_train.shape)
print(X_test.shape)
```

    (426, 30)
    (143, 30)
    


```python
train_df = pd.merge(X_train, y_train, left_index=True, right_index=True, how='inner')
train_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>radius_mean</th>
      <th>texture_mean</th>
      <th>perimeter_mean</th>
      <th>area_mean</th>
      <th>smoothness_mean</th>
      <th>compactness_mean</th>
      <th>concavity_mean</th>
      <th>concave_points_mean</th>
      <th>symmetry_mean</th>
      <th>fractal_dimension_mean</th>
      <th>...</th>
      <th>texture_worst</th>
      <th>perimeter_worst</th>
      <th>area_worst</th>
      <th>smoothness_worst</th>
      <th>compactness_worst</th>
      <th>concavity_worst</th>
      <th>concave_points_worst</th>
      <th>symmetry_worst</th>
      <th>fractal_dimension_worst</th>
      <th>diagnosis</th>
    </tr>
    <tr>
      <th>id</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>925622</th>
      <td>15.22</td>
      <td>30.62</td>
      <td>103.40</td>
      <td>716.9</td>
      <td>0.10480</td>
      <td>0.20870</td>
      <td>0.25500</td>
      <td>0.094290</td>
      <td>0.2128</td>
      <td>0.07152</td>
      <td>...</td>
      <td>42.79</td>
      <td>128.70</td>
      <td>915.0</td>
      <td>0.14170</td>
      <td>0.79170</td>
      <td>1.17000</td>
      <td>0.23560</td>
      <td>0.4089</td>
      <td>0.14090</td>
      <td>M</td>
    </tr>
    <tr>
      <th>8915</th>
      <td>14.96</td>
      <td>19.10</td>
      <td>97.03</td>
      <td>687.3</td>
      <td>0.08992</td>
      <td>0.09823</td>
      <td>0.05940</td>
      <td>0.048190</td>
      <td>0.1879</td>
      <td>0.05852</td>
      <td>...</td>
      <td>26.19</td>
      <td>109.10</td>
      <td>809.8</td>
      <td>0.13130</td>
      <td>0.30300</td>
      <td>0.18040</td>
      <td>0.14890</td>
      <td>0.2962</td>
      <td>0.08472</td>
      <td>B</td>
    </tr>
    <tr>
      <th>848406</th>
      <td>14.68</td>
      <td>20.13</td>
      <td>94.74</td>
      <td>684.5</td>
      <td>0.09867</td>
      <td>0.07200</td>
      <td>0.07395</td>
      <td>0.052590</td>
      <td>0.1586</td>
      <td>0.05922</td>
      <td>...</td>
      <td>30.88</td>
      <td>123.40</td>
      <td>1138.0</td>
      <td>0.14640</td>
      <td>0.18710</td>
      <td>0.29140</td>
      <td>0.16090</td>
      <td>0.3029</td>
      <td>0.08216</td>
      <td>M</td>
    </tr>
    <tr>
      <th>922577</th>
      <td>10.32</td>
      <td>16.35</td>
      <td>65.31</td>
      <td>324.9</td>
      <td>0.09434</td>
      <td>0.04994</td>
      <td>0.01012</td>
      <td>0.005495</td>
      <td>0.1885</td>
      <td>0.06201</td>
      <td>...</td>
      <td>21.77</td>
      <td>71.12</td>
      <td>384.9</td>
      <td>0.12850</td>
      <td>0.08842</td>
      <td>0.04384</td>
      <td>0.02381</td>
      <td>0.2681</td>
      <td>0.07399</td>
      <td>B</td>
    </tr>
    <tr>
      <th>891703</th>
      <td>11.85</td>
      <td>17.46</td>
      <td>75.54</td>
      <td>432.7</td>
      <td>0.08372</td>
      <td>0.05642</td>
      <td>0.02688</td>
      <td>0.022800</td>
      <td>0.1875</td>
      <td>0.05715</td>
      <td>...</td>
      <td>25.75</td>
      <td>84.35</td>
      <td>517.8</td>
      <td>0.13690</td>
      <td>0.17580</td>
      <td>0.13160</td>
      <td>0.09140</td>
      <td>0.3101</td>
      <td>0.07007</td>
      <td>B</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>866674</th>
      <td>19.79</td>
      <td>25.12</td>
      <td>130.40</td>
      <td>1192.0</td>
      <td>0.10150</td>
      <td>0.15890</td>
      <td>0.25450</td>
      <td>0.114900</td>
      <td>0.2202</td>
      <td>0.06113</td>
      <td>...</td>
      <td>33.58</td>
      <td>148.70</td>
      <td>1589.0</td>
      <td>0.12750</td>
      <td>0.38610</td>
      <td>0.56730</td>
      <td>0.17320</td>
      <td>0.3305</td>
      <td>0.08465</td>
      <td>M</td>
    </tr>
    <tr>
      <th>869254</th>
      <td>10.75</td>
      <td>14.97</td>
      <td>68.26</td>
      <td>355.3</td>
      <td>0.07793</td>
      <td>0.05139</td>
      <td>0.02251</td>
      <td>0.007875</td>
      <td>0.1399</td>
      <td>0.05688</td>
      <td>...</td>
      <td>20.72</td>
      <td>77.79</td>
      <td>441.2</td>
      <td>0.10760</td>
      <td>0.12230</td>
      <td>0.09755</td>
      <td>0.03413</td>
      <td>0.2300</td>
      <td>0.06769</td>
      <td>B</td>
    </tr>
    <tr>
      <th>859717</th>
      <td>17.20</td>
      <td>24.52</td>
      <td>114.20</td>
      <td>929.4</td>
      <td>0.10710</td>
      <td>0.18300</td>
      <td>0.16920</td>
      <td>0.079440</td>
      <td>0.1927</td>
      <td>0.06487</td>
      <td>...</td>
      <td>33.82</td>
      <td>151.60</td>
      <td>1681.0</td>
      <td>0.15850</td>
      <td>0.73940</td>
      <td>0.65660</td>
      <td>0.18990</td>
      <td>0.3313</td>
      <td>0.13390</td>
      <td>M</td>
    </tr>
    <tr>
      <th>88249602</th>
      <td>14.03</td>
      <td>21.25</td>
      <td>89.79</td>
      <td>603.4</td>
      <td>0.09070</td>
      <td>0.06945</td>
      <td>0.01462</td>
      <td>0.018960</td>
      <td>0.1517</td>
      <td>0.05835</td>
      <td>...</td>
      <td>30.28</td>
      <td>98.27</td>
      <td>715.5</td>
      <td>0.12870</td>
      <td>0.15130</td>
      <td>0.06231</td>
      <td>0.07963</td>
      <td>0.2226</td>
      <td>0.07617</td>
      <td>B</td>
    </tr>
    <tr>
      <th>854941</th>
      <td>13.03</td>
      <td>18.42</td>
      <td>82.61</td>
      <td>523.8</td>
      <td>0.08983</td>
      <td>0.03766</td>
      <td>0.02562</td>
      <td>0.029230</td>
      <td>0.1467</td>
      <td>0.05863</td>
      <td>...</td>
      <td>22.81</td>
      <td>84.46</td>
      <td>545.9</td>
      <td>0.09701</td>
      <td>0.04619</td>
      <td>0.04833</td>
      <td>0.05013</td>
      <td>0.1987</td>
      <td>0.06169</td>
      <td>B</td>
    </tr>
  </tbody>
</table>
<p>426 rows × 31 columns</p>
</div>




```python
import matplotlib.pyplot as plt
import seaborn as sns
```


```python
fig, axes = plt.subplots(10, 3, figsize=(20, 40))
axes = axes.flatten()

for i in range(30):
    sns.histplot(data=train_df, x=train_df.columns[1], hue='diagnosis', multiple='layer', ax=axes[i])
```


    
![png](/images/2023-01-27-3.5 데이터 축소 - 특징 선택/output_6_0.png)
    


시각화 기반 특징들의 유의미함을 예상
- 유의할 것이라고 예상되는 변수
    - radius_mean, perimeter_mean, area_mean, concave_point_mean, radius_worst, perimeter_worst, area_worst, concave_point_worst 등
- 유의하지 않을 것이라 예상되는 변수
    - smoothness_mean, symmetry_mean, fractal_dimension_mean, texture_se, smoothness_se, symmetry_se, fractal_dimension_se 등
### RFE 기반 주요 특징 선택


```python
from sklearn.svm import SVC
from sklearn.feature_selection import RFE
```


```python
from sklearn.preprocessing import StandardScaler
# 스케일에 민감한 SVM의 특징에 따라 변수의 scaling을 따로 진행

scaler = StandardScaler()
scaler.fit(X_train)
scale_X_train = scaler.transform(X_train)
```


```python
# RFE를 적용할 모델 SVM 지정
estimator_mdl = SVC(kernel="linear")
# SVM 학습 기반의 RFE 실행 및 유의미한 개수 지정 : 5개
svm_rfe = RFE(estimator=estimator_mdl, n_features_to_select=5)

svm_rfe_rst = svm_rfe.fit(scale_X_train, y_train.values.ravel())
svm_rfe_rst.ranking_
# 1로 나온 변수가 최종 유의미한 특징으로 도출된 컬럼
```




    array([17, 23, 11, 16, 20, 25, 14,  9, 21, 13, 12, 24, 18,  1, 15, 10, 22,
            6, 26,  7,  1,  1,  2,  1, 19,  4,  3,  1,  5,  8])




```python
# 도출된 특징 조합으로 test 진행
scale_X_test = scaler.transform(X_test)
prediction = pd.DataFrame(svm_rfe.predict(scale_X_test), columns=['pred_rst'])
prediction
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>pred_rst</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>B</td>
    </tr>
    <tr>
      <th>1</th>
      <td>M</td>
    </tr>
    <tr>
      <th>2</th>
      <td>B</td>
    </tr>
    <tr>
      <th>3</th>
      <td>M</td>
    </tr>
    <tr>
      <th>4</th>
      <td>M</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
    </tr>
    <tr>
      <th>138</th>
      <td>B</td>
    </tr>
    <tr>
      <th>139</th>
      <td>M</td>
    </tr>
    <tr>
      <th>140</th>
      <td>M</td>
    </tr>
    <tr>
      <th>141</th>
      <td>M</td>
    </tr>
    <tr>
      <th>142</th>
      <td>B</td>
    </tr>
  </tbody>
</table>
<p>143 rows × 1 columns</p>
</div>




```python
# 타겟 클래스 데이터 수치형 변경
y_test['diagnosis'] = y_test['diagnosis'].replace('M', 1)
y_test['diagnosis'] = y_test['diagnosis'].replace('B', 0)

prediction['pred_rst'] = prediction['pred_rst'].replace('M', 1)
prediction['pred_rst'] = prediction['pred_rst'].replace('B', 0)

```


```python
# 결과 확인
# accuracy
from sklearn.metrics import accuracy_score
print("accuracy :", round(accuracy_score(y_test['diagnosis'], prediction['pred_rst']), 5))
# auc
from sklearn.metrics import roc_auc_score
print('auc :', round(roc_auc_score(y_test['diagnosis'], prediction['pred_rst']), 5))
```

    accuracy : 0.95105
    auc : 0.94659
    


```python
# RFE를 적용할 모델 SVM 지정
estimator_mdl = SVC(kernel="linear")
# SVM 학습 기반의 RFE 실행 및 유의미한 개수 지정 : 10개
re_svm_rfe = RFE(estimator=estimator_mdl, n_features_to_select=10)
re_svm_rfe_rst = re_svm_rfe.fit(scale_X_train, y_train.values.ravel())

re_prediction = pd.DataFrame(re_svm_rfe_rst.predict(scale_X_test), columns=['pred_rst'])

re_prediction['pred_rst'] = re_prediction['pred_rst'].replace('M', 1)
re_prediction['pred_rst'] = re_prediction['pred_rst'].replace('B', 0)

print("accuracy :", round(accuracy_score(y_test['diagnosis'], re_prediction['pred_rst']), 5))

print('auc :', round(roc_auc_score(y_test['diagnosis'], re_prediction['pred_rst']), 5))
```

    accuracy : 0.93706
    auc : 0.93523
    


```python
chk_var = []
for name, svm_rfe_rst.ranking_ in zip(X_train.columns, svm_rfe_rst.ranking_):
    lst_chk  = [name, svm_rfe_rst.ranking_]
    chk_var.append(lst_chk)

chk_svm_rfe = pd.DataFrame(chk_var, columns = ['feature_names', 'svm_rfe_feature'])
chk_svm_rfe = chk_svm_rfe[chk_svm_rfe["svm_rfe_feature"]==1]
chk_svm_rfe
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature_names</th>
      <th>svm_rfe_feature</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>13</th>
      <td>texture_worst</td>
      <td>1</td>
    </tr>
    <tr>
      <th>20</th>
      <td>radius_worst</td>
      <td>1</td>
    </tr>
    <tr>
      <th>21</th>
      <td>texture_worst</td>
      <td>1</td>
    </tr>
    <tr>
      <th>23</th>
      <td>area_worst</td>
      <td>1</td>
    </tr>
    <tr>
      <th>27</th>
      <td>concave_points_worst</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



### Embedded 기반 특징 선택
- 모델 학습 성능에 기여하는 특징 선택하는 방안


```python
# RF 모형 기반 특징 선택
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier
```


```python
# RF 모형 생성
embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=1))
embeded_rf_selector.fit(scale_X_train, y_train.values.ravel())
```




    SelectFromModel(estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                                     class_weight=None,
                                                     criterion='gini',
                                                     max_depth=None,
                                                     max_features='auto',
                                                     max_leaf_nodes=None,
                                                     max_samples=None,
                                                     min_impurity_decrease=0.0,
                                                     min_impurity_split=None,
                                                     min_samples_leaf=1,
                                                     min_samples_split=2,
                                                     min_weight_fraction_leaf=0.0,
                                                     n_estimators=100, n_jobs=None,
                                                     oob_score=False,
                                                     random_state=1, verbose=0,
                                                     warm_start=False),
                    max_features=None, norm_order=1, prefit=False, threshold=None)




```python
# RF 기반 Embeded 특징 선택 결과
embeded_rf_support = embeded_rf_selector.get_support()
embeded_rf_feature = X_train.loc[:, embeded_rf_support].columns.tolist()
print(str(len(embeded_rf_feature)), 'selected features')
```

    9 selected features
    


```python
embeded_rf_feature
```




    ['radius_mean',
     'perimeter_mean',
     'area_mean',
     'concavity_mean',
     'concave_points_mean',
     'radius_worst',
     'perimeter_worst',
     'area_worst',
     'concave_points_worst']



### Boruta Algorithm
- Boruta Algorithm은 랜덤포레스트 모형 기반 특징 선택 알고리즘
- 기존 데이터를 임의로 복제하여 랜덤변수(shadow 변수)를 생성하고 그 보다 낮은 중요도를 지닌 특징을 제외함


```python
from boruta import BorutaPy
```


```python
# boruta 알고리즘은 랜덤포레스트 모형 기반이므로 먼저 RF 모형 설정
rf = RandomForestClassifier(random_state=1)

# boruta 기반 특징 선택
boruta_selector = BorutaPy(rf, n_estimators="auto", random_state=1)

# 'auto'인 경우는 자동으로 데이터셋 사이즈를 고려하여 자동 설정
boruta_selector.fit(scale_X_train, y_train.values.ravel())
boruta_selector.ranking_
```




    array([1, 1, 1, 1, 1, 1, 1, 1, 3, 5, 1, 5, 1, 1, 7, 1, 1, 1, 6, 3, 1, 1,
           1, 1, 1, 1, 1, 1, 1, 2])




```python
# 최종 선택된 특징 컬럼이 어떤 컬럼인지 확인
chk_var_boruta=[]
for name, boruta_selector.ranking_ in zip(X_train.columns, boruta_selector.ranking_):
    lst_chk = [name, boruta_selector.ranking_]
    chk_var_boruta.append(lst_chk)

chk_boruta = pd.DataFrame(chk_var_boruta, columns=['feature_names', 'boruta_feature'])
chk_boruta = chk_boruta[chk_boruta["boruta_feature"]==1]
chk_boruta
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature_names</th>
      <th>boruta_feature</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>radius_mean</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>texture_mean</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>perimeter_mean</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>area_mean</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>smoothness_mean</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>compactness_mean</td>
      <td>1</td>
    </tr>
    <tr>
      <th>6</th>
      <td>concavity_mean</td>
      <td>1</td>
    </tr>
    <tr>
      <th>7</th>
      <td>concave_points_mean</td>
      <td>1</td>
    </tr>
    <tr>
      <th>10</th>
      <td>radius_se</td>
      <td>1</td>
    </tr>
    <tr>
      <th>12</th>
      <td>perimeter_se</td>
      <td>1</td>
    </tr>
    <tr>
      <th>13</th>
      <td>texture_worst</td>
      <td>1</td>
    </tr>
    <tr>
      <th>15</th>
      <td>compactness_se</td>
      <td>1</td>
    </tr>
    <tr>
      <th>16</th>
      <td>concavity_se</td>
      <td>1</td>
    </tr>
    <tr>
      <th>17</th>
      <td>concave_points_se</td>
      <td>1</td>
    </tr>
    <tr>
      <th>20</th>
      <td>radius_worst</td>
      <td>1</td>
    </tr>
    <tr>
      <th>21</th>
      <td>texture_worst</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22</th>
      <td>perimeter_worst</td>
      <td>1</td>
    </tr>
    <tr>
      <th>23</th>
      <td>area_worst</td>
      <td>1</td>
    </tr>
    <tr>
      <th>24</th>
      <td>smoothness_worst</td>
      <td>1</td>
    </tr>
    <tr>
      <th>25</th>
      <td>compactness_worst</td>
      <td>1</td>
    </tr>
    <tr>
      <th>26</th>
      <td>concavity_worst</td>
      <td>1</td>
    </tr>
    <tr>
      <th>27</th>
      <td>concave_points_worst</td>
      <td>1</td>
    </tr>
    <tr>
      <th>28</th>
      <td>symmetry_worst</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




```python
print(np.shape(chk_boruta))
```

    (23, 2)
    


```python
chk_boruta = pd.DataFrame(chk_var_boruta, columns=['feature_names', 'boruta_feature'])
chk_boruta = chk_boruta[chk_boruta["boruta_feature"]!=1]
chk_boruta
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature_names</th>
      <th>boruta_feature</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>8</th>
      <td>symmetry_mean</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>fractal_dimension_mean</td>
      <td>5</td>
    </tr>
    <tr>
      <th>11</th>
      <td>texture_se</td>
      <td>5</td>
    </tr>
    <tr>
      <th>14</th>
      <td>smoothness_se</td>
      <td>7</td>
    </tr>
    <tr>
      <th>18</th>
      <td>symmetry_se</td>
      <td>6</td>
    </tr>
    <tr>
      <th>19</th>
      <td>fractal_dimension_se</td>
      <td>3</td>
    </tr>
    <tr>
      <th>29</th>
      <td>fractal_dimension_worst</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>




```python

```
