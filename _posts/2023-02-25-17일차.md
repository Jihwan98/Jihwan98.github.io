---
title:  "[KT Aivle 3기 AI] 17일차. 머신러닝 (4)"
author: JIHWAN PARK
categories: [AI & 데이터분석, AIVLE SCHOOL]
tag: [AIVLE SCHOOL, Machine Learning, Python]
math: true
date: 2023-02-25 19:00:00 +0900
last_modified_at: 2023-02-25 19:00:00 +0900
---
> KT Aivle School 3기 AI 17일차 
> - 강사 : 이장래 강사님
> - 주제 : 기본 알고리즘 중 Logistic Regression, SVM과 K-Fold Cross Validation, Hyperparameter 튜닝(Grid Search, Random Search)
> - 내용 :
>   - Logistic Regression, SVM 설명 및 실습
>   - K-Fold Cross Validation 설명 및 실습
>   - Hyperparameter 튜닝 방법 설명 (Grid Search, Random Search)
>   - 모델의 복잡성, 클래스 불균형
{: .prompt-info}

# Logistic Regression
- 학습을 통해 선형 회귀선을 찾는 것이 아니라, 로지스틱 함수에 의해 반환되는 값을 확률로 간주하여 그 확률에 따라 분류를 결정함
- <a href='https://github.com/Jihwan98/aivle_school/blob/main/2023.02.20_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/%EB%B0%B0%EC%9A%B0%EA%B8%B0/ML03_04_%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%ED%9A%8C%EA%B7%80(Iris).ipynb' target='_blank'>Logistic Regression 실습코드(iris)</a>

## 로지스틱 함수

$\large p = \frac{1}{1 + e^{-f(x)}}$

- 시그모이드(Sigmoid) 함수라고도 부름
- 확률 값 p는 선형 판별식 값이 커지면 1, 작아지면 0에 가까운 값이 됨
- $(-\infty, \infty)$ 범위를 갖는 선형 판별식 결과로 $(0, 1)$ 범위의 확률 값을 얻게 됨
- 확률 값 0.5를 기준으로 이진 분류를 수행할 수 있게 됨
> x 데이터가 주어졌을 때 확률을 예측하는 로지스틱 회귀분석은 학습 데이터를 잘 설명하는 선형 판별식의 기울기(a)와 절편(b)를 찾는 문제


# SVM (Support Vector Machine)
- 분류를 위한 기준선, 즉 결정 경계선(Decision Boundary)을 찾는 알고리즘
- SVM 성능을 높이기 위해 <mark>정규화 작업</mark>이 필요
- 분류 문제와 회귀 문제 모두에 사용 사능(SVC, SVR)
- **결정 경계 (Decision Boundary)** : 서로 다른 분륫값을 결정하는 경계
- **서포트 벡터(Support Vector)** : 결정 경계선과 가장 가까운 데이터 포인트
- **마진(Margin)** :
    - 서포트 벡터와 결정 경계 사이의 거리
    - <u>마진을 최대로 하는 결정 경계를 찾는 것</u>이 SVM의 목표
    - 마진이 클수록 새로운 데이터에 대해 안정적으로 분류할 가능성이 높음
- <a href='https://github.com/Jihwan98/aivle_school/blob/main/2023.02.20_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/%EB%B0%B0%EC%9A%B0%EA%B8%B0/REF02_SVM%EC%9D%B4%ED%95%B4.ipynb' target='_blank'>SVM 이해 실습코드</a>
- <a href='https://github.com/Jihwan98/aivle_school/blob/main/2023.02.20_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/%EB%B0%B0%EC%9A%B0%EA%B8%B0/ML03_05_SVM(Admission)%20.ipynb' target='_blank'>SVM 실습코드(Admission)</a>

![svm](https://user-images.githubusercontent.com/76936390/221353698-982cc154-aa7e-4f34-a70f-970a9dec0d45.png)

## 비용 (C)
- 약간의 오류를 허용하기 위해 비용(C)이라는 변수를 사용
- 비용을 낮게 잡으면 (이상치들이 있을 가능성을 크게 봄)
    - 마진을 높이고 에러를 증가시키는 결정 경계선을 만듦 (Soft Margin) → 과소적합 위험
- 비용을 높게 잡으면 (이상치들이 있을 가능성을 작게 봄)
    - 마진은 낮추고 에러를 감소시키는 결정 경계선을 만듦 (Hard Margin) → 과대적합 위험
- SVM에서 적절한 비용 값을 찾는 과정이 매우 중요

## 커널트릭
- 직선으로 분류할 수 없는 경우 **커널 트릭**을 통해 분류
- 저차원 데이터를 고차원 데이터로 옮긴 것과 같은 효과를 줘서 결정 경계선을 찾음
- rbf 커널(기본값), Poly, Linear
- rbf와 poly의 경우 gamma 설정이 필요

## 감마 (gamma)
- 모델이 생성하는 경계가 복잡해지는 정도
- 값이 클 수록 경계가 복잡해지고, 값이 낮을 수록 경계가 단순해짐

# K-분할 교차 검증 (K-Fold Cross Validation)
- 모든 데이터가 평가에 한 번, 학습에 k-1번 사용
- k는 2 이상
- <a href='https://github.com/Jihwan98/aivle_school/blob/main/2023.02.20_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/%EB%B0%B0%EC%9A%B0%EA%B8%B0/ML03_05_SVM(Admission)%20.ipynb' target='_blank'>K-Fold Cross Validation 실습코드(Diabetes)</a>

# Hyperparameter
- 알고리즘을 사용하여 모델링 할 때, 최적화하기 위해 조절할 수 있는 옵션
- 정답은 없음! 다양한 시도가 중요하다. (Grid Search, Random Search)

## 모델 별 Hyperparameter 예시
### **KNN**
- k 값 (n_neighbors)
- 거리 계산법 (metric)

### **Decision Tree**
- max_depth : 트리의 최대 깊이 제한
- min_samples_leaf : leaf가 되귀 위한 최소한의 샘플 데이터 수 (default : 1)
- min_samples_split : 노드를 분할하기 위한 최소한의 샘플 데이터 수(default : 2)

## Grid Search & Random Search
- Grid Search : 설정한 범위의 모든 값에 대한 결과 확인
- Random Search : 설정한 범위에서 설정한 개수 만큼만 랜덤하게 뽑아서 결과 확인

- → Random Search로 구하고, 그 근처를 Grid Search로 진행 하는 방법
- → Grid Search로 진행 한 후 해당 범위의 중앙에 값이 나오면 쓰고~ 범위 끝 쪽 값으로 나오면 범위를 그 방향으로 늘려서 또 돌려보고~ 하는 방법
- → 최선의 파라미터를 찾도록 다양한 방법 활용

- grid search model 에서 `model.cv_results_` 속성에 성능 테스트와 관련된 많은 정보가 포함되어있다.
  - `model.cv_results_['mean_test_score']`: 테스트로 얻은 성능
  - `model.best_params_`: 최적의 파라미터
  - `model.best_score_`: 최고의 성능

- <a href='https://github.com/Jihwan98/aivle_school/blob/main/2023.02.20_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/%EB%B0%B0%EC%9A%B0%EA%B8%B0/ML04_03_GridSearch(%EA%B2%B0%EC%A0%95%ED%8A%B8%EB%A6%AC%2CBoston).ipynb' target='_blank'>Grid Search 실습코드(결정트리, Boston)</a>
- <a href='https://github.com/Jihwan98/aivle_school/blob/main/2023.02.20_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/%EC%9D%B5%ED%9E%88%EA%B8%B0/%EC%8B%A4%EC%8A%B504_06_GridSearch(%EA%B2%B0%EC%A0%95%ED%8A%B8%EB%A6%AC%2CMobile).ipynb' target='_blank'>Grid Search 실습코드(결정트리, Mobile)</a>

# 모델의 복잡성 
- 모델이 복잡하다는 것은
  - 학습 데이터에 너무 치중한 나머지 실제 평가시 성능이 좋지 않은 것을 의미
  - 즉, 과적합 위험이 있는 것을 의미
- Linear Regression : 독립 변수가 많을 수록
- KNN : k값, 즉 n_neighbors 값이 작을 수록
- Decision Tree : max_depth가 클 수록 복잡한 모델이 됨
- 결국 우리는 적절한 복잡성을 갖는 모델을 만들어야 함
- 너무 단순해도 안되고, 너무 복잡해도 안됨
- <a href='https://github.com/Jihwan98/aivle_school/blob/main/2023.02.20_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/%EB%B0%B0%EC%9A%B0%EA%B8%B0/REF12_KNN%EB%B3%B5%EC%9E%A1%EB%8F%84.ipynb' target='_blank'>KNN 복잡도 실습코드</a>

# 클래스 불균형
- 정답 클래스의 값이 불균형하게 있을 경우 (ex: 0 - 100개, 1 - 10개)
- 1에 대한 recall이 떨어짐. 우리는 1에 대한 recall을 올리고 싶다.
- under sampling : 개수가 많은 데이터를 개수가 적은 데이터의 수에 맞춤 (0 - 10개, 1 - 10개)
- over sampling : 개수가 적은 데이터를 개수가 많은 데이터의 수에 맞춤 (0 - 100개, 1 - 100개)
- class_weight = ‘balanced’ : 모델의 weight를 조절하여 맞춤
- <a href='https://github.com/Jihwan98/aivle_school/blob/main/2023.02.20_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5%EC%9E%90%EB%A3%8C/%EB%B0%B0%EC%9A%B0%EA%B8%B0/REF05_%ED%81%B4%EB%9E%98%EC%8A%A4%EB%B6%88%EA%B7%A0%ED%98%95(Attrition).ipynb' target='_blank'>클래스 불균형 관련 코드</a>