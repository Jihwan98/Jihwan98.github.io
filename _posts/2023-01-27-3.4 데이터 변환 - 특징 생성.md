---
layout: single
title:  "[사전학습] 3.4 데이터 변환 - 특징 생성"
excerpt: "3.4 데이터 변환 - 특징 생성 실습 코드"
categories:
    - Aivle_Prestudy
tag: [Aivle_Prestudy, Aivle, Python, Jupyter Notebook, Numpy, Pandas]

date: 2023-01-27
last_modified_at: 2023-01-27
---


# 3. 데이터 전처리 이해와 실무
## 3.1 데이터 변환 : 특징 생성
### 특징 생성 (Feature Creation)
- 원본 데이터의 조합/변환 등을 기반하여 새로운 특징들을 구축 및 생성하는 방법  
=> 원본 데이터로 **특징을 새롭게 생성**하여 분석 과정 내 **성능과 효율성** 확보하고자 함
#### 목적 및 필요성
- 품질 확보 : 가공을 거치지 않은 Raw 데이터 활용 기반의 모델링은 품질 확보 어려움
- 최적화된 형태 변환 : 효과적인 Feature를 확보하는 것이 데이터 분석 내 가장 중요한 과정임
### 특징 생성 방안
1. 범주 인코딩
- 크게 Nominal(순서가 없는)과 Ordinal(순서가 있는) 형식으로 나뉘는 범주형 변수
- **숫자가 아닌 범주 변수 값을 숫자로 표현**하고 모델링에 적용하기 위한 과정
2. 결합 및 분해
- 데이터 셋의 **변수들의 조합을 기반**으로 새로운 특징을 구축하는 방법
- 변수 간의 **연산 혹은 분해**를 통해 새로운 특징을 구축하고 입력 변수로 모델링에 적용
3. 차원 축소
- 원본 데이터로부터 새로운 특징의 집합을 생성하는 것
- **고차원** 원시 데이터 셋을 **저차원으로 차원 축소**하도록 새로운 특징을 생성하는 방식
#### 1. 범주 인코딩
범주형 데이터의 알고리즘 적용을 위한 수치형 변환
##### One-hot Encoding
- 순서의 의미를 지니지 않은 범주형 변수를 처리하는 대표적 방법
- **k개의 범주**를 지닌 범주형 변수를 **k개의 변수**로 변환
#### 2-1. 결합 기반 특징 생성
변수 간의 결합을 통해 새로운 의미를 지닌 특징을 생성
- Add / Divide / Subtract / Multiply
#### 2-2. 분해 기반 특징 생성
변수의 분해를 통해 새로운 의미를 지닌 특징을 생성
- 특정 변수 활용 기반의 새로운 의미를 파악할 수 있는 특징을 생성하는 방법
- 도메인 지식 및 일반적 개념 기반으로 생성 가능
ex) 시간 -> 시간대, 요일 등..
#### 3. 차원 축소 목적 특징 생성
3-1. 변수들이 지닌 정보를 최대한 확보하는 저차원 데이터로 생성
##### PCA(Principal Component Analysis)
- 서로 연관된 변수들이 관측되었을 때, 원본 데이터 분산 기반의 특징을 생성
- 주성분 간의 서로 독립을 이루도록 구성 (상관관계가 없도록 구성)
3-2. 군집 분석 기반의 고차원 데이터를 하나의 특징으로 차원 축소
##### Featurization via Clustering
- 고차원 데이터를 군집 분석을 기반으로 특징의 개수를 하나의 특징(군집 결과)으로 축소
- 이렇게 획득한 군집 결과 특징을 분류/회귀 등 문제 해결을 위한 입력 변수로 할용(Stacking 방법)
- 즉, 원본 데이터 내 **여러 개의 특징을 하나의 특징**으로 축소하여 **모델 연산 비용 감소 추구**

## 실습
샘플 데이터 필요


```python
import numpy as np
import pandas as pd
```


```python
data = pd.read_csv("./data/encoding_sample_data.csv", encoding='cp949')
data.head()
```


```python
data['city'].value_counts()
```

### 1. OneHot Encoding


```python
# city라는 범주형 변수 one_hot Encoding
# Pandas의 get_dummies 함수를 활용 쉽게 구혀 ㄴ가능

encoding_data = data.copy()
encoding_data = pd.get_dummies(encoding_data, columns=['city'])
encoding_data.head()
```

### 2. 결합 및 분해 기반 특징 생성


```python
# 시간대 별 파악 목적
creation_data = data.copy()
creation_data.info()
```


```python
# date 컬럼을 datetime 형식으로 변환
creation_data['date'] = pd.to_datetime(creation_data['date'])
creation_data.head()
```


```python
creation_data.info()
```


```python
creation_data['year'] = creation_data['date'].dt.year #연도
creation_data['month'] = creation_data['date'].dt.month #월
creation_data['day'] = creation_data['date'].dt.day #일
creation_data['hour'] = creation_data['date'].dt.hour #시간
creation_data['dayofweek'] = creation_data['date'].dt.dayofweek #요일 (월 = 0)
creation_data.head()
```


```python
# AM PM
creation_data['ampm'] = 'AM'
creation_data.loc[creation_data['hour'] > 12, 'ampm'] = 'PM'
creation_data
```

### 3. 차원 축소 기반 특징 생성 (1) : PCA (주성분 분석)
- 여러 개의 변수를 지닌 고차원 데이터를 저차원으로 변환하도록 주성분들을 생성하는 알고리즘
- 원 변수들이 지닌 정보를 최대한 확보하는 저차원 데이터로 생성하는 방법


```python
cancer = pd.read_csv('./data/wdbc.data', header=None)

# 데이터 컬럼명 지정
cancer.columns = [
    "id", "diagnosis", "radius_mean", "texture_mean", "perimeter_mean", "area_mean", "smoothness_mean", "compactness_mean", 
    "concavity_mean", "concave_points_mean", "symmetry_mean", "fractal_dimension_mean", "radius_se", "texture_se",
    "perimeter_se", "texture_worst", "smoothness_se", "compactness_se", "concavity_se", "concave_points_se", "symmetry_se",
    "fractal_dimension_se", "radius_worst", "texture_worst", "perimeter_worst", "area_worst", "smoothness_worst", "compactness_worst", 
    "concavity_worst", "concave_points_worst", "symmetry_worst", "fractal_dimension_worst"
]

# ID를 Index화
cancer = cancer.set_index('id')
cancer
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>diagnosis</th>
      <th>radius_mean</th>
      <th>texture_mean</th>
      <th>perimeter_mean</th>
      <th>area_mean</th>
      <th>smoothness_mean</th>
      <th>compactness_mean</th>
      <th>concavity_mean</th>
      <th>concave_points_mean</th>
      <th>symmetry_mean</th>
      <th>...</th>
      <th>radius_worst</th>
      <th>texture_worst</th>
      <th>perimeter_worst</th>
      <th>area_worst</th>
      <th>smoothness_worst</th>
      <th>compactness_worst</th>
      <th>concavity_worst</th>
      <th>concave_points_worst</th>
      <th>symmetry_worst</th>
      <th>fractal_dimension_worst</th>
    </tr>
    <tr>
      <th>id</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>842302</th>
      <td>M</td>
      <td>17.99</td>
      <td>10.38</td>
      <td>122.80</td>
      <td>1001.0</td>
      <td>0.11840</td>
      <td>0.27760</td>
      <td>0.30010</td>
      <td>0.14710</td>
      <td>0.2419</td>
      <td>...</td>
      <td>25.380</td>
      <td>17.33</td>
      <td>184.60</td>
      <td>2019.0</td>
      <td>0.16220</td>
      <td>0.66560</td>
      <td>0.7119</td>
      <td>0.2654</td>
      <td>0.4601</td>
      <td>0.11890</td>
    </tr>
    <tr>
      <th>842517</th>
      <td>M</td>
      <td>20.57</td>
      <td>17.77</td>
      <td>132.90</td>
      <td>1326.0</td>
      <td>0.08474</td>
      <td>0.07864</td>
      <td>0.08690</td>
      <td>0.07017</td>
      <td>0.1812</td>
      <td>...</td>
      <td>24.990</td>
      <td>23.41</td>
      <td>158.80</td>
      <td>1956.0</td>
      <td>0.12380</td>
      <td>0.18660</td>
      <td>0.2416</td>
      <td>0.1860</td>
      <td>0.2750</td>
      <td>0.08902</td>
    </tr>
    <tr>
      <th>84300903</th>
      <td>M</td>
      <td>19.69</td>
      <td>21.25</td>
      <td>130.00</td>
      <td>1203.0</td>
      <td>0.10960</td>
      <td>0.15990</td>
      <td>0.19740</td>
      <td>0.12790</td>
      <td>0.2069</td>
      <td>...</td>
      <td>23.570</td>
      <td>25.53</td>
      <td>152.50</td>
      <td>1709.0</td>
      <td>0.14440</td>
      <td>0.42450</td>
      <td>0.4504</td>
      <td>0.2430</td>
      <td>0.3613</td>
      <td>0.08758</td>
    </tr>
    <tr>
      <th>84348301</th>
      <td>M</td>
      <td>11.42</td>
      <td>20.38</td>
      <td>77.58</td>
      <td>386.1</td>
      <td>0.14250</td>
      <td>0.28390</td>
      <td>0.24140</td>
      <td>0.10520</td>
      <td>0.2597</td>
      <td>...</td>
      <td>14.910</td>
      <td>26.50</td>
      <td>98.87</td>
      <td>567.7</td>
      <td>0.20980</td>
      <td>0.86630</td>
      <td>0.6869</td>
      <td>0.2575</td>
      <td>0.6638</td>
      <td>0.17300</td>
    </tr>
    <tr>
      <th>84358402</th>
      <td>M</td>
      <td>20.29</td>
      <td>14.34</td>
      <td>135.10</td>
      <td>1297.0</td>
      <td>0.10030</td>
      <td>0.13280</td>
      <td>0.19800</td>
      <td>0.10430</td>
      <td>0.1809</td>
      <td>...</td>
      <td>22.540</td>
      <td>16.67</td>
      <td>152.20</td>
      <td>1575.0</td>
      <td>0.13740</td>
      <td>0.20500</td>
      <td>0.4000</td>
      <td>0.1625</td>
      <td>0.2364</td>
      <td>0.07678</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>926424</th>
      <td>M</td>
      <td>21.56</td>
      <td>22.39</td>
      <td>142.00</td>
      <td>1479.0</td>
      <td>0.11100</td>
      <td>0.11590</td>
      <td>0.24390</td>
      <td>0.13890</td>
      <td>0.1726</td>
      <td>...</td>
      <td>25.450</td>
      <td>26.40</td>
      <td>166.10</td>
      <td>2027.0</td>
      <td>0.14100</td>
      <td>0.21130</td>
      <td>0.4107</td>
      <td>0.2216</td>
      <td>0.2060</td>
      <td>0.07115</td>
    </tr>
    <tr>
      <th>926682</th>
      <td>M</td>
      <td>20.13</td>
      <td>28.25</td>
      <td>131.20</td>
      <td>1261.0</td>
      <td>0.09780</td>
      <td>0.10340</td>
      <td>0.14400</td>
      <td>0.09791</td>
      <td>0.1752</td>
      <td>...</td>
      <td>23.690</td>
      <td>38.25</td>
      <td>155.00</td>
      <td>1731.0</td>
      <td>0.11660</td>
      <td>0.19220</td>
      <td>0.3215</td>
      <td>0.1628</td>
      <td>0.2572</td>
      <td>0.06637</td>
    </tr>
    <tr>
      <th>926954</th>
      <td>M</td>
      <td>16.60</td>
      <td>28.08</td>
      <td>108.30</td>
      <td>858.1</td>
      <td>0.08455</td>
      <td>0.10230</td>
      <td>0.09251</td>
      <td>0.05302</td>
      <td>0.1590</td>
      <td>...</td>
      <td>18.980</td>
      <td>34.12</td>
      <td>126.70</td>
      <td>1124.0</td>
      <td>0.11390</td>
      <td>0.30940</td>
      <td>0.3403</td>
      <td>0.1418</td>
      <td>0.2218</td>
      <td>0.07820</td>
    </tr>
    <tr>
      <th>927241</th>
      <td>M</td>
      <td>20.60</td>
      <td>29.33</td>
      <td>140.10</td>
      <td>1265.0</td>
      <td>0.11780</td>
      <td>0.27700</td>
      <td>0.35140</td>
      <td>0.15200</td>
      <td>0.2397</td>
      <td>...</td>
      <td>25.740</td>
      <td>39.42</td>
      <td>184.60</td>
      <td>1821.0</td>
      <td>0.16500</td>
      <td>0.86810</td>
      <td>0.9387</td>
      <td>0.2650</td>
      <td>0.4087</td>
      <td>0.12400</td>
    </tr>
    <tr>
      <th>92751</th>
      <td>B</td>
      <td>7.76</td>
      <td>24.54</td>
      <td>47.92</td>
      <td>181.0</td>
      <td>0.05263</td>
      <td>0.04362</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.1587</td>
      <td>...</td>
      <td>9.456</td>
      <td>30.37</td>
      <td>59.16</td>
      <td>268.6</td>
      <td>0.08996</td>
      <td>0.06444</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>0.2871</td>
      <td>0.07039</td>
    </tr>
  </tbody>
</table>
<p>569 rows × 31 columns</p>
</div>




```python
# input 및 Target 구분
input_df = cancer.drop(['diagnosis'], axis=1)
print(np.shape(input_df))

target_df = cancer[['diagnosis']]
print(np.shape(target_df))
```

    (569, 30)
    (569, 1)
    


```python
# 30개의 독립변수로 이루어진 데이터를 주성분 분석
# 주성분 분석 수행 이전, 각 변수의 스케일이 서로 다르기 때문에 표준화 수행
from sklearn.preprocessing import StandardScaler
std_scaler = StandardScaler()

std_scaler.fit(input_df)
input_scaled = std_scaler.transform(input_df)

input_scaled
```




    array([[ 1.09706398, -2.07333501,  1.26993369, ...,  2.29607613,
             2.75062224,  1.93701461],
           [ 1.82982061, -0.35363241,  1.68595471, ...,  1.0870843 ,
            -0.24388967,  0.28118999],
           [ 1.57988811,  0.45618695,  1.56650313, ...,  1.95500035,
             1.152255  ,  0.20139121],
           ...,
           [ 0.70228425,  2.0455738 ,  0.67267578, ...,  0.41406869,
            -1.10454895, -0.31840916],
           [ 1.83834103,  2.33645719,  1.98252415, ...,  2.28998549,
             1.91908301,  2.21963528],
           [-1.80840125,  1.22179204, -1.81438851, ..., -1.74506282,
            -0.04813821, -0.75120669]])




```python
# 주성분 분석 수행
from sklearn.decomposition import PCA

# 두 개 주성분만 유지시키도록 수행
# 30개 변수의 데이터를 2개의 주성분으로 남도록 변환
pca = PCA(n_components=2)
pca.fit(input_scaled)
X_pca = pca.transform(input_scaled)
X_pca
```




    array([[ 9.19283683,  1.94858307],
           [ 2.3878018 , -3.76817174],
           [ 5.73389628, -1.0751738 ],
           ...,
           [ 1.25617928, -1.90229671],
           [10.37479406,  1.67201011],
           [-5.4752433 , -0.67063679]])




```python
# PCA 수행된 input 데이터 확인
X_pca_df = pd.DataFrame(X_pca, columns=['pc1', 'pc2'])
X_pca_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>pc1</th>
      <th>pc2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>9.192837</td>
      <td>1.948583</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2.387802</td>
      <td>-3.768172</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5.733896</td>
      <td>-1.075174</td>
    </tr>
    <tr>
      <th>3</th>
      <td>7.122953</td>
      <td>10.275589</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3.935302</td>
      <td>-1.948072</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>564</th>
      <td>6.439315</td>
      <td>-3.576817</td>
    </tr>
    <tr>
      <th>565</th>
      <td>3.793382</td>
      <td>-3.584048</td>
    </tr>
    <tr>
      <th>566</th>
      <td>1.256179</td>
      <td>-1.902297</td>
    </tr>
    <tr>
      <th>567</th>
      <td>10.374794</td>
      <td>1.672010</td>
    </tr>
    <tr>
      <th>568</th>
      <td>-5.475243</td>
      <td>-0.670637</td>
    </tr>
  </tbody>
</table>
<p>569 rows × 2 columns</p>
</div>



- 30개의 변수를 지닌 데이터가 2개의 특징들로 압축되었음


```python
import matplotlib.pyplot as plt
import seaborn as sns
```


```python
# 산점도로 2개의 주성분을 시각화
ax = sns.scatterplot(x='pc1', y='pc2', data=X_pca_df)
```


    
![png](\images\2023-01-27-3.4 데이터 변환 - 특징 생성\output_20_0.png)
    



```python
# Target과 확인을 위해 주성분 분석을 수행한 Input 데이터와 기존 Target 데이터를 Merge
target_df = target_df.reset_index()
pca_df = pd.merge(X_pca_df, target_df, left_index=True, right_index=True, how='inner')
pca_df = pca_df[['pc1', 'pc2', 'diagnosis']]
pca_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>pc1</th>
      <th>pc2</th>
      <th>diagnosis</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>9.192837</td>
      <td>1.948583</td>
      <td>M</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2.387802</td>
      <td>-3.768172</td>
      <td>M</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5.733896</td>
      <td>-1.075174</td>
      <td>M</td>
    </tr>
    <tr>
      <th>3</th>
      <td>7.122953</td>
      <td>10.275589</td>
      <td>M</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3.935302</td>
      <td>-1.948072</td>
      <td>M</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>564</th>
      <td>6.439315</td>
      <td>-3.576817</td>
      <td>M</td>
    </tr>
    <tr>
      <th>565</th>
      <td>3.793382</td>
      <td>-3.584048</td>
      <td>M</td>
    </tr>
    <tr>
      <th>566</th>
      <td>1.256179</td>
      <td>-1.902297</td>
      <td>M</td>
    </tr>
    <tr>
      <th>567</th>
      <td>10.374794</td>
      <td>1.672010</td>
      <td>M</td>
    </tr>
    <tr>
      <th>568</th>
      <td>-5.475243</td>
      <td>-0.670637</td>
      <td>B</td>
    </tr>
  </tbody>
</table>
<p>569 rows × 3 columns</p>
</div>




```python
# 클래스를 색깔로 구분하여 처음 두개의 주성분으로 Target과 비교
ax = sns.scatterplot(x='pc1', y='pc2', hue='diagnosis', data=pca_df, palette=['green', 'red'])
```


    
![png](\images\2023-01-27-3.4 데이터 변환 - 특징 생성\output_22_0.png)
    


- 실제 모델링에 적용하기 위해 효율적으로 활용 가능한 주성분 분석
- 특히, record 및 컬럼이 많은 경우 모델링 연산비용이 많이 들게 되므로 효율적인 차원 축소 기반의 특징을 생성하는 것이 분석 과정 내 필요한 과정

- 주성분을 선택하는 다른 방안
- 유지시킬 주성분 개수가 아닌 분산의 설명가능 수준을 비율로 선택 가능
    - pca = PCA(n_components=0.8):
    - 주성분의 수는 전체 분산의 최소 80% 수준에서 자동으로 선택
- 이를 통해 수치를 변경하면서 추출되는 주성분의 수 확인 가능하며, 이는 분산에 기초한 주성분 개수를 선택하는 부분에서 얼마나 많은 주성분을 사용할 것인지 확인해야 할 때 사용 가능


```python
# 전체 분산의 최소 80% 수준에서 설명하는 수준의 주성분 확보
pca = PCA(n_components=0.8)
pca.fit(input_scaled)
X_pca = pca.transform(input_scaled)
X_pca_df = pd.DataFrame(X_pca)
X_pca_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>9.192837</td>
      <td>1.948583</td>
      <td>-1.123166</td>
      <td>3.633731</td>
      <td>-1.195110</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2.387802</td>
      <td>-3.768172</td>
      <td>-0.529293</td>
      <td>1.118264</td>
      <td>0.621775</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5.733896</td>
      <td>-1.075174</td>
      <td>-0.551748</td>
      <td>0.912083</td>
      <td>-0.177086</td>
    </tr>
    <tr>
      <th>3</th>
      <td>7.122953</td>
      <td>10.275589</td>
      <td>-3.232790</td>
      <td>0.152547</td>
      <td>-2.960878</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3.935302</td>
      <td>-1.948072</td>
      <td>1.389767</td>
      <td>2.940639</td>
      <td>0.546747</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>564</th>
      <td>6.439315</td>
      <td>-3.576817</td>
      <td>2.459487</td>
      <td>1.177314</td>
      <td>-0.074824</td>
    </tr>
    <tr>
      <th>565</th>
      <td>3.793382</td>
      <td>-3.584048</td>
      <td>2.088476</td>
      <td>-2.506028</td>
      <td>-0.510723</td>
    </tr>
    <tr>
      <th>566</th>
      <td>1.256179</td>
      <td>-1.902297</td>
      <td>0.562731</td>
      <td>-2.089227</td>
      <td>1.809991</td>
    </tr>
    <tr>
      <th>567</th>
      <td>10.374794</td>
      <td>1.672010</td>
      <td>-1.877029</td>
      <td>-2.356031</td>
      <td>-0.033742</td>
    </tr>
    <tr>
      <th>568</th>
      <td>-5.475243</td>
      <td>-0.670637</td>
      <td>1.490443</td>
      <td>-2.299157</td>
      <td>-0.184703</td>
    </tr>
  </tbody>
</table>
<p>569 rows × 5 columns</p>
</div>



### 3. 차원 축소 기반 특징 생성 (2) : clustering (군집 분석)
- 여러 개의 변수를 하나의 변수(군집결과)로 변환 차원 축소


```python
from sklearn.cluster import KMeans
```


```python
# 일부 변수만 선택 (30개 변수 중 15개의 변수만 임의로 선정)
subset_df = input_df.iloc[:, 0:15]
subset_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>radius_mean</th>
      <th>texture_mean</th>
      <th>perimeter_mean</th>
      <th>area_mean</th>
      <th>smoothness_mean</th>
      <th>compactness_mean</th>
      <th>concavity_mean</th>
      <th>concave_points_mean</th>
      <th>symmetry_mean</th>
      <th>fractal_dimension_mean</th>
      <th>radius_se</th>
      <th>texture_se</th>
      <th>perimeter_se</th>
      <th>texture_worst</th>
      <th>smoothness_se</th>
    </tr>
    <tr>
      <th>id</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>842302</th>
      <td>17.99</td>
      <td>10.38</td>
      <td>122.80</td>
      <td>1001.0</td>
      <td>0.11840</td>
      <td>0.27760</td>
      <td>0.30010</td>
      <td>0.14710</td>
      <td>0.2419</td>
      <td>0.07871</td>
      <td>1.0950</td>
      <td>0.9053</td>
      <td>8.589</td>
      <td>153.40</td>
      <td>0.006399</td>
    </tr>
    <tr>
      <th>842517</th>
      <td>20.57</td>
      <td>17.77</td>
      <td>132.90</td>
      <td>1326.0</td>
      <td>0.08474</td>
      <td>0.07864</td>
      <td>0.08690</td>
      <td>0.07017</td>
      <td>0.1812</td>
      <td>0.05667</td>
      <td>0.5435</td>
      <td>0.7339</td>
      <td>3.398</td>
      <td>74.08</td>
      <td>0.005225</td>
    </tr>
    <tr>
      <th>84300903</th>
      <td>19.69</td>
      <td>21.25</td>
      <td>130.00</td>
      <td>1203.0</td>
      <td>0.10960</td>
      <td>0.15990</td>
      <td>0.19740</td>
      <td>0.12790</td>
      <td>0.2069</td>
      <td>0.05999</td>
      <td>0.7456</td>
      <td>0.7869</td>
      <td>4.585</td>
      <td>94.03</td>
      <td>0.006150</td>
    </tr>
    <tr>
      <th>84348301</th>
      <td>11.42</td>
      <td>20.38</td>
      <td>77.58</td>
      <td>386.1</td>
      <td>0.14250</td>
      <td>0.28390</td>
      <td>0.24140</td>
      <td>0.10520</td>
      <td>0.2597</td>
      <td>0.09744</td>
      <td>0.4956</td>
      <td>1.1560</td>
      <td>3.445</td>
      <td>27.23</td>
      <td>0.009110</td>
    </tr>
    <tr>
      <th>84358402</th>
      <td>20.29</td>
      <td>14.34</td>
      <td>135.10</td>
      <td>1297.0</td>
      <td>0.10030</td>
      <td>0.13280</td>
      <td>0.19800</td>
      <td>0.10430</td>
      <td>0.1809</td>
      <td>0.05883</td>
      <td>0.7572</td>
      <td>0.7813</td>
      <td>5.438</td>
      <td>94.44</td>
      <td>0.011490</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>926424</th>
      <td>21.56</td>
      <td>22.39</td>
      <td>142.00</td>
      <td>1479.0</td>
      <td>0.11100</td>
      <td>0.11590</td>
      <td>0.24390</td>
      <td>0.13890</td>
      <td>0.1726</td>
      <td>0.05623</td>
      <td>1.1760</td>
      <td>1.2560</td>
      <td>7.673</td>
      <td>158.70</td>
      <td>0.010300</td>
    </tr>
    <tr>
      <th>926682</th>
      <td>20.13</td>
      <td>28.25</td>
      <td>131.20</td>
      <td>1261.0</td>
      <td>0.09780</td>
      <td>0.10340</td>
      <td>0.14400</td>
      <td>0.09791</td>
      <td>0.1752</td>
      <td>0.05533</td>
      <td>0.7655</td>
      <td>2.4630</td>
      <td>5.203</td>
      <td>99.04</td>
      <td>0.005769</td>
    </tr>
    <tr>
      <th>926954</th>
      <td>16.60</td>
      <td>28.08</td>
      <td>108.30</td>
      <td>858.1</td>
      <td>0.08455</td>
      <td>0.10230</td>
      <td>0.09251</td>
      <td>0.05302</td>
      <td>0.1590</td>
      <td>0.05648</td>
      <td>0.4564</td>
      <td>1.0750</td>
      <td>3.425</td>
      <td>48.55</td>
      <td>0.005903</td>
    </tr>
    <tr>
      <th>927241</th>
      <td>20.60</td>
      <td>29.33</td>
      <td>140.10</td>
      <td>1265.0</td>
      <td>0.11780</td>
      <td>0.27700</td>
      <td>0.35140</td>
      <td>0.15200</td>
      <td>0.2397</td>
      <td>0.07016</td>
      <td>0.7260</td>
      <td>1.5950</td>
      <td>5.772</td>
      <td>86.22</td>
      <td>0.006522</td>
    </tr>
    <tr>
      <th>92751</th>
      <td>7.76</td>
      <td>24.54</td>
      <td>47.92</td>
      <td>181.0</td>
      <td>0.05263</td>
      <td>0.04362</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.1587</td>
      <td>0.05884</td>
      <td>0.3857</td>
      <td>1.4280</td>
      <td>2.548</td>
      <td>19.15</td>
      <td>0.007189</td>
    </tr>
  </tbody>
</table>
<p>569 rows × 15 columns</p>
</div>




```python
std_scaler.fit(subset_df)
subset_input_scaled = std_scaler.transform(subset_df)
subset_input_scaled
```




    array([[ 1.09706398, -2.07333501,  1.26993369, ...,  2.83303087,
             2.48757756, -0.21400165],
           [ 1.82982061, -0.35363241,  1.68595471, ...,  0.26332697,
             0.74240195, -0.60535085],
           [ 1.57988811,  0.45618695,  1.56650313, ...,  0.8509283 ,
             1.18133606, -0.29700501],
           ...,
           [ 0.70228425,  2.0455738 ,  0.67267578, ...,  0.27669279,
             0.1806983 , -0.37934168],
           [ 1.83834103,  2.33645719,  1.98252415, ...,  1.43852964,
             1.0095027 , -0.17299998],
           [-1.80840125,  1.22179204, -1.81438851, ..., -0.15744905,
            -0.46615196,  0.04934236]])




```python
# K-means 클러스터링 활용
# 군집 Label 수 설정
k = 5
model = KMeans(n_clusters=k, random_state=10)
```


```python
model.fit(subset_input_scaled)

target_df['cluster'] = model.fit_predict(subset_input_scaled)
```

    c:\Users\zxwlg\miniconda3\envs\dicom\lib\site-packages\sklearn\cluster\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.
      warnings.warn(
    c:\Users\zxwlg\miniconda3\envs\dicom\lib\site-packages\sklearn\cluster\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.
      warnings.warn(
    


```python
target_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>index</th>
      <th>id</th>
      <th>diagnosis</th>
      <th>cluster</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>842302</td>
      <td>M</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>842517</td>
      <td>M</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>84300903</td>
      <td>M</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>84348301</td>
      <td>M</td>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>84358402</td>
      <td>M</td>
      <td>2</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>564</th>
      <td>564</td>
      <td>926424</td>
      <td>M</td>
      <td>0</td>
    </tr>
    <tr>
      <th>565</th>
      <td>565</td>
      <td>926682</td>
      <td>M</td>
      <td>2</td>
    </tr>
    <tr>
      <th>566</th>
      <td>566</td>
      <td>926954</td>
      <td>M</td>
      <td>2</td>
    </tr>
    <tr>
      <th>567</th>
      <td>567</td>
      <td>927241</td>
      <td>M</td>
      <td>0</td>
    </tr>
    <tr>
      <th>568</th>
      <td>568</td>
      <td>92751</td>
      <td>B</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
<p>569 rows × 4 columns</p>
</div>




```python
pd.crosstab(target_df.diagnosis, target_df.cluster)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>cluster</th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
    </tr>
    <tr>
      <th>diagnosis</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>B</th>
      <td>0</td>
      <td>111</td>
      <td>0</td>
      <td>229</td>
      <td>17</td>
    </tr>
    <tr>
      <th>M</th>
      <td>36</td>
      <td>5</td>
      <td>93</td>
      <td>26</td>
      <td>52</td>
    </tr>
  </tbody>
</table>
</div>



- 임의의 15개 변수만을 활용한 하나의 특징(군집 결과)이 Target 구분에 효과적일 것임을 예측 가능함
- 이처럼 많은 변수를 하나의 특징으로 구성하고, 입력 데이터의 차원을 줄인다면 모델 연산 비용 절감에 효과적
